{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arimac_by_me.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3Kzfto-ihdPn",
        "ZM78AGPYNi0g",
        "u2zQBkCTKZbM",
        "pUxEN_Pz9-3N",
        "lbnGFOVv9o2i",
        "wa6gdTOp9hWC",
        "w0kfcgOVcCO-",
        "muJQHVHAH-u-",
        "g6qAo7q9cNlP",
        "zRcgO7_cQ-oy",
        "hldDUn3LEBHp"
      ],
      "toc_visible": true,
      "mount_file_id": "1tSh4xmTafMD88IbBM5rDQwoVr3GdvS5q",
      "authorship_tag": "ABX9TyOImG0kne48JWKOxLv7Nepf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VEENA9/Leaf-/blob/master/Arimac_by_me.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kzfto-ihdPn"
      },
      "source": [
        "#lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOigv-x_b3wI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edb66527-81de-4b6a-c5ed-fdcb305d90b5"
      },
      "source": [
        "!pip3 install Pillow\n",
        "!pip3 install pytesseract\n",
        "!pip3 install pdf2image\n",
        "!sudo apt-get install tesseract-ocr\n",
        "!apt-get install poppler-utils \n",
        "!pip install python-poppler"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.7/dist-packages (0.3.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pytesseract) (7.1.2)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.7/dist-packages (1.15.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pdf2image) (7.1.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.00~git2288-10f4998a-2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (0.62.0-2ubuntu2.12).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Collecting python-poppler\n",
            "  Using cached https://files.pythonhosted.org/packages/38/77/8f7b5c8ff2c0a7c3afbb3dc8d2342ef2e3ef48053a467e02913e18b73dc6/python-poppler-0.2.2.tar.gz\n",
            "Building wheels for collected packages: python-poppler\n",
            "  Building wheel for python-poppler (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for python-poppler\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for python-poppler\n",
            "Failed to build python-poppler\n",
            "Installing collected packages: python-poppler\n",
            "    Running setup.py install for python-poppler ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-1cng4g07/python-poppler/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-1cng4g07/python-poppler/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-w7q3r_7d/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM78AGPYNi0g"
      },
      "source": [
        "#Filtred DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOfOnBjBDbTX"
      },
      "source": [
        "# get data from image\n",
        "def save_df(df_img,last_blok):\n",
        "\n",
        "  table_Cv = df_img\n",
        "  # image data to DF\n",
        "  table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "  # image data to DF\n",
        "  # add headera same as 1st row\n",
        "  table_Cv_df1 = table_Cv_df.rename(columns=table_Cv_df.iloc[0])\n",
        "  # remove 1st row\n",
        "  table_Cv_df1  = table_Cv_df1[1:]\n",
        "\n",
        "  # filter DF (empty text, left and top 0, remove \"e\":line)\n",
        "  Df_filter_f = table_Cv_df1.loc[(table_Cv_df1.text != ' ') & (table_Cv_df1.left != '0')&(table_Cv_df1.text !='  ')&\n",
        "                              (table_Cv_df1.top != '0')& (table_Cv_df1.text != 'e')& (table_Cv_df1.text != \"\")& \n",
        "                              (table_Cv_df1.conf != \"0\"),['block_num','par_num','width','height',\"conf\",'text']]\n",
        "  \n",
        "  # Df_filter1 = table_Cv_df1.loc[(table_Cv_df1.text.str.len() > 3) & (pd.to_numeric(table_Cv_df1['conf']) > 74)]\n",
        "  # print(Df_filter_f)\n",
        "  # (Df_filter_f.text.str.len() > 3)\n",
        "  Df_f_1 = Df_filter_f[pd.to_numeric(Df_filter_f['conf']) <= 75]\n",
        "  Df_f_2 = Df_filter_f[Df_filter_f.text.str.len() < 3]  \n",
        "  Df_f_3 = Df_f_1.join(Df_f_2, lsuffix='_caller', rsuffix='_other').dropna()\n",
        "  Df_filter1 = Df_filter_f.drop(Df_f_3.index.values.tolist())\n",
        "  Df_filter1 = Df_filter1.rename(columns={'block_num': 'block_n'})\n",
        "  add_no = pd.to_numeric(Df_filter1[\"block_n\"]) +last_blok\n",
        "  Df_filter1['block_num'] = add_no\n",
        "  Df_filter1 = Df_filter1.dropna()\n",
        "  next_first_blok = Df_filter1[\"block_num\"].iloc[0]\n",
        "  # print(Df_filter1) \n",
        "  return Df_filter1,next_first_blok                          "
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uU6WTrVOCQM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2zQBkCTKZbM"
      },
      "source": [
        "#return head an name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnI__1_EOsM6"
      },
      "source": [
        "def find_head(dff):\n",
        "  Df_filter = dff  \n",
        "  # Find key heading\n",
        "  key_word = [\"CONTACT\",\"Education\",\"Project\",\"EXPERIENCE\",\"Referees\",\"Skill\",\"About\",\"Award\",\"Professional\",\"INTEREST\",\n",
        "              \"ACTIVITIES\",\"AWARDS\",\"PUBLICATION\",\"Activity\",\"Hobbies\",\"Profile\",\"work\",\"QUALIFICATION\"]\n",
        "  awoid_key = [\"www\",\"@\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\",\"=\",\"_\",\",\",\"&\"]\n",
        "  # get guessed heading  \n",
        "  pattern = '|'.join(key_word)\n",
        "  guessed_heading_df =Df_filter[Df_filter['text'].str.contains(pattern,na=False, case=False)]\n",
        "  # get list of guessed head lock num\n",
        "  # print(guessed_heading_df[\"text\"].tolist())\n",
        "  block_num_h_guessd = guessed_heading_df[\"block_num\"].values.tolist()\n",
        "  # print(guessed_heading_df)\n",
        "\n",
        "  # awoid symbols\n",
        "  awoid_pattern = '|'.join(awoid_key)\n",
        "  df_awoid_symbol =Df_filter[Df_filter['text'].str.contains(awoid_pattern,na=False, case=False)] \n",
        "  # print(df_awoid_symbol[\"block_num\"].tolist())\n",
        "  # print(df_awoid_symbol[\"text\"])\n",
        "  df_awoid = Df_filter[~Df_filter['block_num'].isin(df_awoid_symbol[\"block_num\"].values.tolist())]\n",
        "  \n",
        "  # print(df_awoid[\"text\"].tolist())\n",
        "  # get the guessed heading block number\n",
        "    # get original (awoid symbol) block list\n",
        "  block_num_all = df_awoid[\"block_num\"].values.tolist()\n",
        "  # print(block_num_all)\n",
        "  # find raws based same block_num \n",
        "  full_heading_df = df_awoid[df_awoid['block_num'].isin(block_num_h_guessd)]\n",
        "  # print(full_heading_df[\"block_num\"].values.tolist())\n",
        "  # block number from original df\n",
        "  same_block_no=full_heading_df['block_num'].values.tolist()\n",
        "  # print(same_block_no)\n",
        "  # 1 or 2 blocks items (we assume heading must on 1 or 2 words) (get 1 or 2 block word from originl df)\n",
        "  must_head_blok_list = [item for item, count in collections.Counter(same_block_no).items() if (count <= 3)]\n",
        "  # print(must_head_blok_list)\n",
        "  # find df of must head \n",
        "  one_or_two_blok_heading1 = Df_filter[Df_filter['block_num'].isin(must_head_blok_list)]\n",
        "  # print(one_or_two_blok_heading1)\n",
        "\n",
        "  # ///////////// -----|^ find gessed all heading\n",
        "\n",
        "  \n",
        "  # key waigth////// in one_or_two_blok_heading \n",
        "  numaric_w = pd.to_numeric(df_awoid['width'])\n",
        "  numaric_h = pd.to_numeric(df_awoid['height'])\n",
        "  df_waigth = numaric_w * numaric_h\n",
        "  # charector count\n",
        "  D_text_count = df_awoid['text'].str.len()\n",
        "  df_key_waight_all =df_waigth / D_text_count\n",
        "  # print(df_key_waight_all)\n",
        "  # //////////\n",
        "  \n",
        "  \n",
        "\n",
        "  # /////////////// get gussed heading waight and find minimum number of k_waight \n",
        "  numaric_w_h = pd.to_numeric(one_or_two_blok_heading1['width'])\n",
        "  numaric_h_h = pd.to_numeric(one_or_two_blok_heading1['height'])\n",
        "  df_waigth_head = numaric_w_h * numaric_h_h\n",
        "  # print(df_waigth_all.values.tolist())\n",
        "  D_text_count_head = one_or_two_blok_heading1['text'].str.len()\n",
        "  df_key_waight_head = df_waigth_head / D_text_count_head\n",
        "  avg_key_waight = df_key_waight_head.min(skipna=True)\n",
        "  # print(avg_key_waight)\n",
        "  # ///////////////\n",
        "\n",
        "  # print(Df_filter[Df_filter['key waigth'] > 25860 ])\n",
        "  Df_waight_filter = df_key_waight_all[df_key_waight_all >= avg_key_waight] \n",
        "  # print(Df_waight_filter)\n",
        "\n",
        "  Df_waight_filter_text = (pd.concat([Df_waight_filter, df_awoid.reindex(Df_waight_filter.index)], axis=1)).dropna()\n",
        "  # print(Df_waight_filter_text)\n",
        "  \n",
        "  # 1 or 2 or 3 line contain heading : validate\n",
        "  block_num_h1 = Df_waight_filter_text[\"block_num\"].values.tolist()\n",
        "  # print(block_num_h1)\n",
        "\n",
        "  # must_head_blok_list = [item for item, count in collections.Counter(block_num_h1).items() if (count <= 3)]\n",
        "  # print(block_num_h1)\n",
        "  full_heading_df1 = Df_filter[Df_filter['block_num'].isin(block_num_h1)]\n",
        "  fewline_head = full_heading_df1[\"block_num\"].tolist()\n",
        "  # print(fewline_head)\n",
        "  must_head_blok_list = [item for item, count in collections.Counter(fewline_head).items() if (count <= 3)]\n",
        "  df_headind = Df_filter[Df_filter['block_num'].isin(must_head_blok_list)]\n",
        "\n",
        "\n",
        "\n",
        "  # NAME\n",
        "  # //////\n",
        "  df_len_for_name = len(Df_filter) / 3\n",
        "  Df_filter_name = Df_filter.loc[:df_len_for_name]\n",
        "  numaric_w = pd.to_numeric(Df_filter_name['width'])\n",
        "  numaric_h = pd.to_numeric(Df_filter_name['height'])\n",
        "  df_waigth = numaric_w * numaric_h\n",
        "  # charector count\n",
        "  D_text_count = Df_filter['text'].str.len()\n",
        "  df_key_waight_all =df_waigth / D_text_count\n",
        "  # //////\n",
        "  Df_waight_filter = df_key_waight_all[df_key_waight_all >= avg_key_waight] \n",
        "  # print(Df_waight_filter)\n",
        "  \n",
        "  Df_waight_filter_text = (pd.concat([Df_waight_filter, Df_filter_name], axis=1)).dropna()\n",
        "  \n",
        "  pattern = '|'.join(key_word)\n",
        "  guessed_heading_df =Df_waight_filter_text[~Df_waight_filter_text['text'].str.contains(pattern,na=False, case=False)]\n",
        "  awoid_pattern = '|'.join(awoid_key)\n",
        "  df_awoid_symbol =guessed_heading_df[~guessed_heading_df['text'].str.contains(awoid_pattern,na=False, case=False)]\n",
        "  fewline_head_name = df_awoid_symbol[\"block_num\"].tolist()\n",
        "  # print(df_awoid_symbol)\n",
        "  \n",
        "  # print(must_head_blok_list1)\n",
        "  must_head_blok_list2 = Df_filter_name[Df_filter_name['block_num'].isin(fewline_head_name)]\n",
        "  must_head_blok_list1 = [item for item, count in collections.Counter(must_head_blok_list2[\"block_num\"].tolist()).items() if (count <= 7)]\n",
        "  # print(must_head_blok_list1)\n",
        "  must_head_blok_list = Df_filter_name[Df_filter_name['block_num'].isin(must_head_blok_list1)]\n",
        "  # must_head_blok_list = (pd.merge_asof(df2, Df_filter_name,on=index))\n",
        "  # print(must_head_blok_list)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "# /////// big word find\n",
        "  numaric_w_n = pd.to_numeric(must_head_blok_list['width'])\n",
        "  numaric_h_n = pd.to_numeric(must_head_blok_list['height'])\n",
        "  df_waigth_n = numaric_w_n * numaric_h_n\n",
        "  # charector count\n",
        "  D_text_count = must_head_blok_list['text'].str.len()\n",
        "  df_key_waight_all = df_waigth_n / D_text_count\n",
        "  # print(df_key_waight_all)\n",
        "\n",
        "  must_head_blok_list3 = must_head_blok_list.loc[df_key_waight_all.index.tolist()]\n",
        "  # print(must_head_blok_list3)\n",
        "  pattern = '|'.join(key_word)\n",
        "  must_head_blok_list =must_head_blok_list3[~must_head_blok_list3['text'].str.contains(pattern,na=False, case=False)]\n",
        "  # print(must_head_blok_list)\n",
        "  df_key_waight_all1 = df_key_waight_all.loc[must_head_blok_list.index.values.tolist()]\n",
        "  name_index = df_key_waight_all1.idxmax()  \n",
        "  # print(df_key_waight_all1)\n",
        "  block_num = must_head_blok_list.loc[name_index].block_num\n",
        "  # print(block_num)\n",
        "  # find raws based same block_num \n",
        "  df2 =must_head_blok_list.loc[must_head_blok_list[\"block_num\"] == block_num]\n",
        "  # print(df2)\n",
        "  # get text of same raw and mearg as a string.\n",
        "  ln = df2['text'].values\n",
        "  print(' '.join(ln))\n",
        "  \n",
        "  # 1 or 2 or 3 line contain heading : validate\n",
        "  # block_num_h1 = Df_waight_filter_text[\"block_num\"].values.tolist()\n",
        "  # pattern = '|'.join(key_word)\n",
        "  # df_headind_n =df_headind[~df_headind['text'].str.contains(pattern,na=False, case=False)]  \n",
        "\n",
        "  return df_headind"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUxEN_Pz9-3N"
      },
      "source": [
        "#find head (heading, no heading)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IglUh0nwato6"
      },
      "source": [
        "def details(Df_filter,Df_head):\n",
        "  word = \"EXPERIENCE\"\n",
        "  head_index = Df_head[Df_head['text'].str.contains(word, na=False, case=False)]\n",
        "  # print(head_index)\n",
        "  if head_index.empty == True:\n",
        "    print(word,\" is not in the Heading list.\")\n",
        "    no_heading(Df_filter,Df_head,word)\n",
        "  else:\n",
        "    print(word, \"is in the Heading list.\")\n",
        "    heading(Df_filter,Df_head,word,head_index)"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbnGFOVv9o2i"
      },
      "source": [
        "# heading = TRUE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VukqALZKRZD"
      },
      "source": [
        "# heading = TRUE\n",
        "def heading(Df_filter,Df_head,word,head_index):\n",
        "    \n",
        "  # get Details(df) between two heading\n",
        "  start_index = head_index.index.values[0] \n",
        "  df_head_index_list = Df_head.index.values.tolist()\n",
        "  end_index = df_head_index_list[(df_head_index_list.index(start_index)) % len(df_head_index_list)] \n",
        "\n",
        "  if end_index==start_index:\n",
        "    print(word,\" is last heading\")\n",
        "      # end_index = None\n",
        "      # end_index = int()\n",
        "\n",
        "    df_data_word = Df_filter.loc[start_index+1 : ]\n",
        "    \n",
        "    # get number of block count , between two heading(ovvoru blocklayum periya k_waightku uriya word.a eduthu, athu Key_wordku\n",
        "    # samana irukkanu paarthu = enda athodaye stop pannanum, illana adutha head vara vaasikanum)\n",
        "    # get block numbers\n",
        "    blok_tuple = (df_data_word[\"block_num\"].drop_duplicates())\n",
        "    # print(blok_tuple)\n",
        "    blok_len = len(blok_tuple)\n",
        "    # print(blok_len)\n",
        "    b = 0\n",
        "    while (b < blok_len):\n",
        "      blok_num=blok_tuple.iloc[b]\n",
        "      a_blok_text_set = df_data_word[df_data_word[\"block_num\"].isin([blok_num])]\n",
        "      numaric_w_w = pd.to_numeric(a_blok_text_set['width'])\n",
        "      numaric_h_w = pd.to_numeric(a_blok_text_set['height'])\n",
        "      df_waigth_word = numaric_w_w * numaric_h_w\n",
        "      D_text_count_head = a_blok_text_set['text'].str.len()\n",
        "      df_key_waight_word = df_waigth_word / D_text_count_head\n",
        "      max_key_waight = df_key_waight_word.idxmax()\n",
        "      max_w_word_a_blok = df_data_word[\"text\"].loc[[max_key_waight]]\n",
        "      # print(max_w_word_a_blok)\n",
        "      \n",
        "      key_word = [\"Education\",\"Project\",\"Referees\",\"Skill\",\"Award\",\"Professional\",\"INTEREST\",\n",
        "                  \"ACTIVITIES\",\"PUBLICATION\",\"Activity\",\"Hobbies\"]\n",
        "\n",
        "      pattern = '|'.join(key_word)\n",
        "      df_if_in_word = max_w_word_a_blok.str.contains(pattern,na=False, case=False).values[0]\n",
        "      # print(max_w_word_a_blok.str.contains(pattern,na=False, case=False))\n",
        "      if df_if_in_word == True:\n",
        "        end_index1 = max_w_word_a_blok.index.values[0] \n",
        "        df_data_word_finl = Df_filter.loc[start_index+1 : end_index1-1]\n",
        "        break \n",
        "      else:\n",
        "        df_data_word_finl = Df_filter.loc[start_index+1 :]   \n",
        "      b += 1\n",
        "\n",
        "  else:\n",
        "    print(word,\" is center heading\")\n",
        "    df_data_word = Df_filter.loc[start_index+1 : end_index-1 ]\n",
        "    \n",
        "    \n",
        "    # get number of block count , between two heading(ovvoru blocklayum periya k_waightku uriya word.a eduthu, athu Key_wordku\n",
        "    # samana irukkanu paarthu = enda athodaye stop pannanum, illana adutha head vara vaasikanum)\n",
        "    # get block numbers\n",
        "    blok_tuple = (df_data_word[\"block_num\"].drop_duplicates())\n",
        "    # print(blok_tuple)\n",
        "    blok_len = len(blok_tuple)\n",
        "    # print(blok_len)\n",
        "    b = 0\n",
        "    while (b < blok_len):\n",
        "      blok_num=blok_tuple.iloc[b]\n",
        "      a_blok_text_set = df_data_word[df_data_word[\"block_num\"].isin([blok_num])]\n",
        "      numaric_w_w = pd.to_numeric(a_blok_text_set['width'])\n",
        "      numaric_h_w = pd.to_numeric(a_blok_text_set['height'])\n",
        "      df_waigth_word = numaric_w_w * numaric_h_w\n",
        "      D_text_count_head = a_blok_text_set['text'].str.len()\n",
        "      df_key_waight_word = df_waigth_word / D_text_count_head\n",
        "      max_key_waight = df_key_waight_word.idxmax()\n",
        "      max_w_word_a_blok = df_data_word[\"text\"].loc[[max_key_waight]]\n",
        "      \n",
        "      key_word = [\"Education\",\"Project\",\"Referees\",\"Skill\",\"Award\",\"Professional\",\"INTEREST\",\n",
        "                  \"ACTIVITIES\",\"PUBLICATION\",\"Activity\",\"Hobbies\"]\n",
        "\n",
        "      pattern = '|'.join(key_word)\n",
        "      df_if_in_word = max_w_word_a_blok.str.contains(pattern,na=False, case=False).values[0]\n",
        "     \n",
        "      if df_if_in_word == True:\n",
        "        end_index1 = max_w_word_a_blok.index.values[0] \n",
        "        df_data_word_finl = Df_filter.loc[start_index+1 : end_index1-1]\n",
        "        break \n",
        "      else:\n",
        "        df_data_word_finl = Df_filter.loc[start_index+1 :end_index-1]   \n",
        "      b += 1\n",
        "\n",
        "\n",
        "  # df_data_word_finl = Df_filter.loc[start_index+1 : end_index]\n",
        "  ln = df_data_word_finl['text'].values\n",
        "  details_text = '\\n'.join(' '.join(ln).split('. '))\n",
        "  print(details_text)"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa6gdTOp9hWC"
      },
      "source": [
        "# Heading Fales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP7T-yr9QHz0"
      },
      "source": [
        "# Heading Fales \n",
        "# word.a thedi eduthu, athila periya k_waight irukratha select panni\n",
        "# athukku aduthatha ulla head.a thedi eduththu, idaila ullatha df aakkanum, next ,Heading True.la ulla pola seiyanum.\n",
        "\n",
        "def no_heading(Df_filter,Df_head,word):\n",
        "\n",
        "  head_index_tuple = Df_filter[Df_filter['text'].str.contains(word, na=False, case=False)]\n",
        "  # print(head_index_tuple)\n",
        "\n",
        "  if head_index_tuple.empty:\n",
        "    print(word,\" is not in the cv\")    \n",
        "\n",
        "  else:\n",
        "    numaric_w_w = pd.to_numeric(head_index_tuple['width'])\n",
        "    numaric_h_w = pd.to_numeric(head_index_tuple['height'])\n",
        "    df_waigth_word = numaric_w_w * numaric_h_w\n",
        "    D_text_count_head = head_index_tuple['text'].str.len()\n",
        "    df_key_waight_word = df_waigth_word / D_text_count_head\n",
        "    # print(df_key_waight_word)\n",
        "    max_key_waight = df_key_waight_word.idxmax()\n",
        "    # print(max_key_waight)\n",
        "    max_w_word_a_blok = Df_filter[\"text\"].loc[[max_key_waight]]\n",
        "\n",
        "    big_word_index = max_w_word_a_blok.index[0]\n",
        "    start_index = big_word_index\n",
        "    # find next head in Df_head\n",
        "      # find last head index\n",
        "    # print(Df_head[\"text\"].tolist())\n",
        "    head_index = Df_head.index.tolist() \n",
        "    last_head_index = Df_head.iloc[[-1]].index[0]\n",
        "    # print(last_head_index , big_word_index )\n",
        "    \n",
        "    # big word last head or belove to head\n",
        "    if last_head_index <= big_word_index:\n",
        "      df_data_word_bigword_l = Df_filter.loc[big_word_index+1 :]  \n",
        "      df_data_word_finl = Df_filter.loc[start_index+1 :]\n",
        "      print(\"word in last heading\")\n",
        "\n",
        "       # get number of block count , between two heading(ovvoru blocklayum periya k_waightku uriya word.a eduthu, athu Key_wordku\n",
        "      # samana irukkanu paarthu = enda athodaye stop pannanum, illana adutha head vara vaasikanum)\n",
        "      # get block numbers\n",
        "      blok_tuple = (df_data_word_bigword[\"block_num\"].drop_duplicates())\n",
        "      # print(df_data_word[df_data_word[\"block_num\"].isin([17])])\n",
        "      blok_len = len(blok_tuple)\n",
        "      # print(blok_len)\n",
        "      b = 0\n",
        "      while (b < blok_len):\n",
        "        blok_num=blok_tuple.iloc[b]\n",
        "        a_blok_text_set = df_data_word_bigword[df_data_word_bigword[\"block_num\"].isin([blok_num])]\n",
        "        numaric_w_w = pd.to_numeric(a_blok_text_set['width'])\n",
        "        numaric_h_w = pd.to_numeric(a_blok_text_set['height'])\n",
        "        df_waigth_word = numaric_w_w * numaric_h_w\n",
        "        D_text_count_head = a_blok_text_set['text'].str.len()\n",
        "        df_key_waight_word = df_waigth_word / D_text_count_head\n",
        "        max_key_waight = df_key_waight_word.idxmax()\n",
        "        max_w_word_a_blok = df_data_word_bigword[\"text\"].loc[[max_key_waight]]\n",
        "        \n",
        "        key_word = [\"Education\",\"Project\",\"Referees\",\"Skill\",\"Award\",\"Professional\",\"INTEREST\",\n",
        "                    \"ACTIVITIES\",\"PUBLICATION\",\"Activity\",\"Hobbies\"]\n",
        "\n",
        "        pattern = '|'.join(key_word)\n",
        "        df_if_in_word = max_w_word_a_blok.str.contains(pattern,na=False, case=False).values[0]\n",
        "        # print(df_if_in_word)\n",
        "        if df_if_in_word == True:\n",
        "          end_index1 = max_w_word_a_blok.index.values[0]\n",
        "          df_data_word_finl = Df_filter.loc[start_index+1 : end_index1-1]           \n",
        "          break  \n",
        "        else:\n",
        "          df_data_word_finl = Df_filter.loc[start_index+1 :]  \n",
        "        b +=1       \n",
        "        \n",
        "\n",
        "    else: \n",
        "      print(\"word in center area\")\n",
        "      next_head_bigword = [i for i in head_index if i > big_word_index][0]\n",
        "      # print(head_index)\n",
        "      # print(big_word_index+1 , next_head_bigword-1 )\n",
        "      df_data_word_bigword = Df_filter.loc[big_word_index+1 : next_head_bigword-1]\n",
        "      # print(df_data_word_bigword)\n",
        "\n",
        "      # get number of block count , between two heading(ovvoru blocklayum periya k_waightku uriya word.a eduthu, athu Key_wordku\n",
        "      # samana irukkanu paarthu = enda athodaye stop pannanum, illana adutha head vara vaasikanum)\n",
        "      # get block numbers\n",
        "      blok_tuple = (df_data_word_bigword[\"block_num\"].drop_duplicates())\n",
        "      # print(df_data_word[df_data_word[\"block_num\"].isin([17])])\n",
        "      blok_len = len(blok_tuple)\n",
        "      # print(blok_len)\n",
        "      b = 0\n",
        "      while (b < blok_len):\n",
        "        blok_num=blok_tuple.iloc[b]\n",
        "        a_blok_text_set = df_data_word_bigword[df_data_word_bigword[\"block_num\"].isin([blok_num])]\n",
        "        numaric_w_w = pd.to_numeric(a_blok_text_set['width'])\n",
        "        numaric_h_w = pd.to_numeric(a_blok_text_set['height'])\n",
        "        df_waigth_word = numaric_w_w * numaric_h_w\n",
        "        D_text_count_head = a_blok_text_set['text'].str.len()\n",
        "        df_key_waight_word = df_waigth_word / D_text_count_head\n",
        "        max_key_waight = df_key_waight_word.idxmax()\n",
        "        max_w_word_a_blok = df_data_word_bigword[\"text\"].loc[[max_key_waight]]\n",
        "        \n",
        "        key_word = [\"Education\",\"Project\",\"Referees\",\"Skill\",\"Award\",\"Professional\",\"INTEREST\",\n",
        "                    \"ACTIVITIES\",\"PUBLICATION\",\"Activity\",\"Hobbies\"]\n",
        "\n",
        "        pattern = '|'.join(key_word)\n",
        "        df_if_in_word = max_w_word_a_blok.str.contains(pattern,na=False, case=False).values[0]\n",
        "        # print(df_if_in_word)\n",
        "        if df_if_in_word == True:\n",
        "          end_index1 = max_w_word_a_blok.index.values[0] \n",
        "          df_data_word_finl = Df_filter.loc[start_index+1 : end_index1-1]\n",
        "          break \n",
        "        else:\n",
        "          end_index = next_head_bigword-1 \n",
        "          df_data_word_finl = Df_filter.loc[start_index+1 : end_index]  \n",
        "        b +=1\n",
        "        \n",
        "    # df_data_word_finl = Df_filter.loc[start_index+1 : end_index]\n",
        "    ln = df_data_word_finl['text'].values\n",
        "    details_text = '\\n'.join(' '.join(ln).split('. '))\n",
        "    print(details_text)  "
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0kfcgOVcCO-"
      },
      "source": [
        "#DF by OCR - main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9TWITHc7jnY"
      },
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pdf2image\n",
        "try:\n",
        "    from PIL import Image\n",
        "except ImportError:\n",
        "    import Image\n",
        "import pytesseract\n",
        "\n",
        "\n",
        "def pdf_to_img(pdf_file):\n",
        "    return pdf2image.convert_from_path('/content/drive/MyDrive/Example/resume_012 (1)-converted.pdf')\n",
        "\n",
        "\n",
        "def ocr_core(file):\n",
        "    df_text = pytesseract.image_to_data(file)\n",
        "    return df_text\n",
        "\n",
        "def head_find(Df_filter1):\n",
        "    dff1 = Df_filter1\n",
        "    Df_head_e = pd.DataFrame([]) \n",
        "    df_head1=find_head(dff1) \n",
        "    Df_head_e = Df_head_e.append(df_head1)\n",
        "    return Df_head_e\n",
        "\n",
        "\n",
        "def print_pages(pdf_file):\n",
        "    images = pdf_to_img(pdf_file)\n",
        "    Df_filter_e = pd.DataFrame([])\n",
        "    # Df_head = pd.DataFrame([])\n",
        "    for pg, img in enumerate(images):\n",
        "        df_img = ocr_core(img)\n",
        "        # dff = save_df(df_img)\n",
        "        if Df_filter_e.empty == True:\n",
        "          df_fill1,next_first_blok=save_df(df_img,0) \n",
        "          Df_filter_e = Df_filter_e.append(df_fill1,ignore_index=True)\n",
        "        else:\n",
        "          last_blok = Df_filter_e[\"block_num\"].iloc[-1]+1\n",
        "          blok_add_no = last_blok - next_first_blok\n",
        "          df_fill1,next_first_blok=save_df(df_img,blok_add_no) \n",
        "          Df_filter_e = Df_filter_e.append(df_fill1,ignore_index=True)\n",
        "        \n",
        "    \n",
        "    Df_head = head_find(Df_filter_e)\n",
        "    details(Df_filter_e,Df_head)\n",
        "    print(Df_head)\n",
        "    # print(Df_filter_e)\n",
        "\n",
        "    # Df_head.to_csv (r'/content/head.csv', index = False, header=True)\n",
        "    Df_filter_e.to_csv (r'/content/data1.csv', index = False, header=True) \n",
        "       \n",
        "print_pages('sample.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muJQHVHAH-u-"
      },
      "source": [
        "# find Details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvWWjU_9IVu_"
      },
      "source": [
        "a, b = print_pages('sample.pdf')\n",
        "Df_head = a\n",
        "Df_filter = b\n",
        "# print(df_head)\n",
        "# print(Df_filter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6qAo7q9cNlP"
      },
      "source": [
        "#moving avg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arDDFvo1sznW"
      },
      "source": [
        "# df_last_values = one_or_two_blok_heading[\"key waigth\"]\n",
        "# print(df_last_values)\n",
        "# Data_point_m= df_last_values.rolling(2, min_periods=1).mean()\n",
        "# Data_point_m_D = Data_point_m.squeeze().tolist()\n",
        "# round_to_whole = [round(num) for num in Data_point_m_D]\n",
        "# print(round_to_whole)\n",
        "# may_reduse=((sum(round_to_whole) / len(round_to_whole)) *0.25)\n",
        "# moving_avg = ((sum(round_to_whole) / len(round_to_whole)))\n",
        "# may_lower = moving_avg - may_reduse\n",
        "# if (may_lower < min(df_last_values)):\n",
        "#   avg_key_waight = min(df_last_values)\n",
        "# else:\n",
        "#   avg_key_waight = min(round_to_whole)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_ukkzoyfVaj"
      },
      "source": [
        "#org"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsMLk_pS-uj1"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import sys\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "import collections\n",
        "\n",
        "# Path of the pdf\n",
        "PDF_file = \"/content/drive/MyDrive/Example/Alagaretnam Elakyaveena.pdf\"\n",
        "\n",
        "'''\n",
        "Part #1 : Converting PDF to images\n",
        "'''\n",
        "\n",
        "# Store all the pages of the PDF in a variable\n",
        "pages = convert_from_path(PDF_file, 500)\n",
        "\n",
        "# Counter to store images of each page of PDF to image\n",
        "image_counter = 1\n",
        "\n",
        "# Iterate through all the pages stored above\n",
        "for page in pages:\n",
        "\n",
        "\t# Declaring filename for each page of PDF as JPG\n",
        "\t# For each page, filename will be:\n",
        "\t# PDF page 1 -> page_1.jpg\n",
        "\t# PDF page 2 -> page_2.jpg\n",
        "\t# PDF page 3 -> page_3.jpg\n",
        "\t# ....\n",
        "\t# PDF page n -> page_n.jpg\n",
        "\tfilename = \"page_\"+str(image_counter)+\".jpg\"\n",
        "\t\n",
        "\t# Save the image of the page in system\n",
        "\tpage.save(filename, 'JPEG')\n",
        "\n",
        "\t# Increment the counter to update filename\n",
        "\timage_counter = image_counter + 1\n",
        "\n",
        "'''\n",
        "Part #2 - Recognizing text from the images using OCR\n",
        "'''\n",
        "\t# 3\n",
        "# Variable to get count of total number of pages\n",
        "filelimit = image_counter-1\n",
        "\n",
        "# Creating a text file to write the output\n",
        "outfile = \"out_text.txt\"\n",
        "\n",
        "# Open the file in append mode so that\n",
        "# All contents of all images are added to the same file\n",
        "f = open(outfile, \"a\")\n",
        "\n",
        "# Iterate from 1 to total number of pages\n",
        "for i in range(1, filelimit + 1):\n",
        "\n",
        "\t# Set filename to recognize text from\n",
        "\t# Again, these files will be:\n",
        "\t# page_1.jpg\n",
        "\t# page_2.jpg\n",
        "\t# ....\n",
        "\t# page_n.jpg\n",
        "\tfilename = \"page_\"+str(i)+\".jpg\"\n",
        "\t\t\n",
        "\t# Recognize the text as string in image using pytesserct\n",
        "\ttext = str(((pytesseract.image_to_string(Image.open(filename)))))\n",
        "\n",
        "\t# The recognized text is stored in variable text\n",
        "\t# Any string processing may be applied on text\n",
        "\t# Here, basic formatting has been done:\n",
        "\t# In many PDFs, at line ending, if a word can't\n",
        "\t# be written fully, a 'hyphen' is added.\n",
        "\t# The rest of the word is written in the next line\n",
        "\t# Eg: This is a sample text this word here GeeksF-\n",
        "\t# orGeeks is half on first line, remaining on next.\n",
        "\t# To remove this, we replace every '-\\n' to ''.\n",
        "\ttext = text.replace('-\\n', '')\t\n",
        "\n",
        "\t# Finally, write the processed text to the file.\n",
        "\tf.write(text)\n",
        "\n",
        "# Close the file after writing all the text.\n",
        "f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PibfYydrYD8Q"
      },
      "source": [
        "#name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKRTaB1wsffe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4b5333-a4d5-4460-eb06-7e32e23f6016"
      },
      "source": [
        "# get data from image\n",
        "table_Cv = pytesseract.image_to_data(Image.open(filename))\n",
        "# image data to DF\n",
        "table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "# print(table_Cv_df)\n",
        "# table_Cv = pytesseract.image_to_data(Image.open(filename))\n",
        "# image data to DF\n",
        "# table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "# add headera same as 1st row\n",
        "table_Cv_df1 = table_Cv_df.rename(columns=table_Cv_df.iloc[0])\n",
        "# remove 1st row\n",
        "table_Cv_df1  = table_Cv_df1[1:]\n",
        "\n",
        "# filter DF (empty text, left and top 0, remove \"e\":line)\n",
        "Df_filter = table_Cv_df1.loc[(table_Cv_df1.text != ' ') & (table_Cv_df1.left != '0') & \n",
        "                             (table_Cv_df1.top != '0')& (table_Cv_df1.text != 'e')& (table_Cv_df1.text != \"\")]\n",
        "name = (Df_filter.width) \n",
        "Df_filter.to_csv (r'/content/amilD.csv', index = False, header=True)\n",
        "# get high widht size as it is a NAME\n",
        "big_word =(max(Df_filter.width.astype(float)))\n",
        "# get the big size word block number\n",
        "block_num = Df_filter[\"block_num\"].iloc[Df_filter[\"width\"].astype(float).argmax()]\n",
        "# find raws based same block_num \n",
        "df2 =Df_filter.loc[Df_filter[\"block_num\"] == block_num]\n",
        "# get text of same raw and mearg as a string.\n",
        "ln = df2['text'].values\n",
        "print(' '.join(ln))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AWARDS & ACHIEVEMENTS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRcgO7_cQ-oy"
      },
      "source": [
        "#word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk8WyisNxveS",
        "outputId": "f72e419a-6852-4488-f239-5b6aad022342"
      },
      "source": [
        "#find the word, get that row tuple\n",
        "word = \"Experience\"\n",
        "# word = \"Education\"\n",
        "# find word tuple\n",
        "df_tuple_word=Df_filter[Df_filter['text'].str.contains(word, na=False, case=False)]\n",
        "# index of the word in original DF and next index of filter df////////////////\n",
        "# array of filtred df index\n",
        "place_num =Df_filter.index.values\n",
        "# index of original df\n",
        "word_index = df_tuple_word.index.values[0]\n",
        "# original index number on array\n",
        "x = np.where(place_num == word_index)\n",
        "# next to filterd df index of word in array\n",
        "index_new = x[0][0] + 1\n",
        "# index of original df\n",
        "next_indx = place_num[index_new]\n",
        "# block number of next to word bloc number\n",
        "next_blok_num = Df_filter.block_num[next_indx]\n",
        "# print(next_blok_num)\n",
        "# if heading and details in same block (check next raw is in same with word block number)\n",
        "if (int(next_blok_num) == int(df_tuple_word['block_num'])):\n",
        "  df3_b_num = int(df_tuple_word['block_num']) \n",
        "  \n",
        "else:\n",
        "  # sum one to find next raw num (experiance details in start from next line of experiance)\n",
        "  df3_b_num = int(df_tuple_word['block_num']) + 1\n",
        "# get df that: experiance data in same block number\n",
        "df_same_bloc_num =df_tuple_word.loc[df_tuple_word[\"block_num\"] == str(df3_b_num)]\n",
        "print(df_same_bloc_num)\n",
        "# get totel line of expariance\n",
        "# print(df_same_bloc_num.par_num.astype(float))\n",
        "try:\n",
        "  max_line_num = max(df_same_bloc_num.par_num.astype(float))\n",
        "except:\n",
        "  max_line_num = 1\n",
        "\n",
        "# loop for get line by line basd on par_num\n",
        "p_n = 1\n",
        "print(max_line_num)\n",
        "while (p_n <= max_line_num):\n",
        "  # creat df : for one line (filter based same par_num)\n",
        "  df_par_num =df_same_bloc_num.loc[df_same_bloc_num[\"par_num\"] == str(p_n)]\n",
        "  # add word of same par_num\n",
        "  ln2 = df_par_num['text'].values\n",
        "  word_set = ' '.join(ln2)\n",
        "  p_n =p_n+1;  \n",
        "print(word_set)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-6fab16a4cff7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplace_num\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mDf_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# index of original df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_tuple_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# original index number on array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplace_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE_h0egGOP0g"
      },
      "source": [
        "#findHead Algo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hldDUn3LEBHp"
      },
      "source": [
        "#save df1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUqphpCeyqnS"
      },
      "source": [
        "table_Cv = pytesseract.image_to_data(Image.open(filename))\n",
        "# print(table_Cv.shape)\n",
        "table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "print(table_Cv_df)\n",
        "table_Cv_df.to_csv (r'/content/export_dataframe.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}