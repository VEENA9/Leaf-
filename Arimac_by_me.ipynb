{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arimac_by_me.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3Kzfto-ihdPn",
        "ZM78AGPYNi0g",
        "pUxEN_Pz9-3N",
        "lbnGFOVv9o2i",
        "wa6gdTOp9hWC",
        "muJQHVHAH-u-",
        "g6qAo7q9cNlP",
        "S_ukkzoyfVaj",
        "zRcgO7_cQ-oy",
        "hldDUn3LEBHp"
      ],
      "mount_file_id": "1tSh4xmTafMD88IbBM5rDQwoVr3GdvS5q",
      "authorship_tag": "ABX9TyMUONtiaqOpDJEkl9rbEpsJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VEENA9/Leaf-/blob/master/Arimac_by_me.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kzfto-ihdPn"
      },
      "source": [
        "#lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOigv-x_b3wI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cff8008-065d-41f3-e37b-9f8f640d6421"
      },
      "source": [
        "!pip3 install Pillow\n",
        "!pip3 install pytesseract\n",
        "!pip3 install pdf2image\n",
        "!sudo apt-get install tesseract-ocr\n",
        "!apt-get install poppler-utils \n",
        "!pip install python-poppler"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Collecting pytesseract\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/e6/a4e9fc8a93c1318540e8de6d8d4beb5749b7960388a7c7f27799fc2dd016/pytesseract-0.3.7.tar.gz\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pytesseract) (7.1.2)\n",
            "Building wheels for collected packages: pytesseract\n",
            "  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytesseract: filename=pytesseract-0.3.7-py2.py3-none-any.whl size=13945 sha256=72c9e3177b5811d745d3b4d701624578f6e28c203961f7ef694a3e30f2955357\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/20/7e/1dd0daad1575d5260916bb1e9781246430647adaef4b3ca3b3\n",
            "Successfully built pytesseract\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.7\n",
            "Collecting pdf2image\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/17/debc231f9bdb499e93389bb8679c7091ee9e4993dd92ed2da18aa896e2b6/pdf2image-1.15.1-py3-none-any.whl\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pdf2image) (7.1.2)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.15.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 4,795 kB of archives.\n",
            "After this operation, 15.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB]\n",
            "Fetched 4,795 kB in 1s (3,807 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 160706 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 154 kB of archives.\n",
            "After this operation, 613 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 poppler-utils amd64 0.62.0-2ubuntu2.12 [154 kB]\n",
            "Fetched 154 kB in 1s (263 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 160753 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_0.62.0-2ubuntu2.12_amd64.deb ...\n",
            "Unpacking poppler-utils (0.62.0-2ubuntu2.12) ...\n",
            "Setting up poppler-utils (0.62.0-2ubuntu2.12) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting python-poppler\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/77/8f7b5c8ff2c0a7c3afbb3dc8d2342ef2e3ef48053a467e02913e18b73dc6/python-poppler-0.2.2.tar.gz (595kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 6.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: python-poppler\n",
            "  Building wheel for python-poppler (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for python-poppler\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for python-poppler\n",
            "Failed to build python-poppler\n",
            "Installing collected packages: python-poppler\n",
            "    Running setup.py install for python-poppler ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-ib1ns72s/python-poppler/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-ib1ns72s/python-poppler/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-sgtfe_hd/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM78AGPYNi0g"
      },
      "source": [
        "#Filtred DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOfOnBjBDbTX"
      },
      "source": [
        "# get data from image\n",
        "def save_df(df_img,last_blok):\n",
        "\n",
        "  table_Cv = df_img\n",
        "  # image data to DF\n",
        "  table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "  # image data to DF\n",
        "  # add headera same as 1st row\n",
        "  table_Cv_df1 = table_Cv_df.rename(columns=table_Cv_df.iloc[0])\n",
        "  # remove 1st row\n",
        "  table_Cv_df1  = table_Cv_df1[1:]\n",
        "\n",
        "  # filter DF (empty text, left and top 0, remove \"e\":line)\n",
        "  Df_filter1 = table_Cv_df1.loc[(table_Cv_df1.text != ' ') & (table_Cv_df1.left != '0') & \n",
        "                              (table_Cv_df1.top != '0')& (table_Cv_df1.text != 'e')& (table_Cv_df1.text != \"\")& (table_Cv_df1.conf != \"0\")]\n",
        "\n",
        "  Df_filter1 = Df_filter1.rename(columns={'block_num': 'block_n'})\n",
        "  add_no = pd.to_numeric(Df_filter1[\"block_n\"]) +last_blok\n",
        "  Df_filter1['block_num'] = add_no\n",
        "  Df_filter1 = Df_filter1.dropna()\n",
        "  next_first_blok = Df_filter1[\"block_num\"].iloc[0]\n",
        "  # print(Df_filter1) \n",
        "  return Df_filter1,next_first_blok                          "
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uU6WTrVOCQM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2zQBkCTKZbM"
      },
      "source": [
        "#return head an name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnI__1_EOsM6"
      },
      "source": [
        "def find_head(dff):\n",
        "  Df_filter = dff  \n",
        "  # Find key heading\n",
        "  key_word = [\"CONTACT\",\"Education\",\"Project\",\"EXPERIENCE\",\"Referees\",\"Skill\",\"About\",\"Award\",\"Professional\",\"INTEREST\",\n",
        "              \"ACTIVITIES\",\"AWARDS\",\"PUBLICATION\",\"Activity\",\"Hobbies\",\"Profile\",\"work\",\"QUALIFICATION\"]\n",
        "  awoid_key = [\"www\",\"@\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\",\":\",\"=\",\"_\",\",\",\"&\"]\n",
        "  # get guessed heading  \n",
        "  pattern = '|'.join(key_word)\n",
        "  guessed_heading_df =Df_filter[Df_filter['text'].str.contains(pattern,na=False, case=False)]\n",
        "  # get list of guessed head lock num\n",
        "  # print(guessed_heading_df[\"text\"].tolist())\n",
        "  block_num_h_guessd = guessed_heading_df[\"block_num\"].values.tolist()\n",
        "  # print(guessed_heading_df)\n",
        "\n",
        "  # awoid symbols\n",
        "  awoid_pattern = '|'.join(awoid_key)\n",
        "  df_awoid_symbol =Df_filter[Df_filter['text'].str.contains(awoid_pattern,na=False, case=False)] \n",
        "  # print(df_awoid_symbol[\"block_num\"].tolist())\n",
        "  # print(df_awoid_symbol[\"text\"])\n",
        "  df_awoid = Df_filter[~Df_filter['block_num'].isin(df_awoid_symbol[\"block_num\"].values.tolist())]\n",
        "  \n",
        "  # print(df_awoid[\"text\"].tolist())\n",
        "  # get the guessed heading block number\n",
        "    # get original (awoid symbol) block list\n",
        "  block_num_all = df_awoid[\"block_num\"].values.tolist()\n",
        "  # print(block_num_all)\n",
        "  # find raws based same block_num \n",
        "  full_heading_df = df_awoid[df_awoid['block_num'].isin(block_num_h_guessd)]\n",
        "  # print(full_heading_df[\"block_num\"].values.tolist())\n",
        "  # block number from original df\n",
        "  same_block_no=full_heading_df['block_num'].values.tolist()\n",
        "  # print(same_block_no)\n",
        "  # 1 or 2 blocks items (we assume heading must on 1 or 2 words) (get 1 or 2 block word from originl df)\n",
        "  must_head_blok_list = [item for item, count in collections.Counter(same_block_no).items() if (count <= 3)]\n",
        "  # print(must_head_blok_list)\n",
        "  # find df of must head \n",
        "  one_or_two_blok_heading1 = Df_filter[Df_filter['block_num'].isin(must_head_blok_list)]\n",
        "  # print(one_or_two_blok_heading1)\n",
        "\n",
        "  # ///////////// -----|^ find gessed all heading\n",
        "\n",
        "  \n",
        "  # key waigth////// in one_or_two_blok_heading \n",
        "  numaric_w = pd.to_numeric(df_awoid['width'])\n",
        "  numaric_h = pd.to_numeric(df_awoid['height'])\n",
        "  df_waigth = numaric_w * numaric_h\n",
        "  # charector count\n",
        "  D_text_count = df_awoid['text'].str.len()\n",
        "  df_key_waight_all =df_waigth / D_text_count\n",
        "  # print(df_key_waight_all)\n",
        "  # //////////\n",
        "  \n",
        "  \n",
        "\n",
        "  # /////////////// get gussed heading waight and find minimum number of k_waight \n",
        "  numaric_w_h = pd.to_numeric(one_or_two_blok_heading1['width'])\n",
        "  numaric_h_h = pd.to_numeric(one_or_two_blok_heading1['height'])\n",
        "  df_waigth_head = numaric_w_h * numaric_h_h\n",
        "  # print(df_waigth_all.values.tolist())\n",
        "  D_text_count_head = one_or_two_blok_heading1['text'].str.len()\n",
        "  df_key_waight_head = df_waigth_head / D_text_count_head\n",
        "  avg_key_waight = df_key_waight_head.min(skipna=True)\n",
        "  # print(avg_key_waight)\n",
        "  # ///////////////\n",
        "\n",
        "  # print(Df_filter[Df_filter['key waigth'] > 25860 ])\n",
        "  Df_waight_filter = df_key_waight_all[df_key_waight_all >= avg_key_waight] \n",
        "  # print(Df_waight_filter)\n",
        "\n",
        "  Df_waight_filter_text = (pd.concat([Df_waight_filter, df_awoid.reindex(Df_waight_filter.index)], axis=1)).dropna()\n",
        "  # print(Df_waight_filter_text)\n",
        "  \n",
        "  # 1 or 2 or 3 line contain heading : validate\n",
        "  block_num_h1 = Df_waight_filter_text[\"block_num\"].values.tolist()\n",
        "  # print(block_num_h1)\n",
        "\n",
        "  # must_head_blok_list = [item for item, count in collections.Counter(block_num_h1).items() if (count <= 3)]\n",
        "  # print(block_num_h1)\n",
        "  full_heading_df1 = Df_filter[Df_filter['block_num'].isin(block_num_h1)]\n",
        "  fewline_head = full_heading_df1[\"block_num\"].tolist()\n",
        "  must_head_blok_list = [item for item, count in collections.Counter(fewline_head).items() if (count <= 3)]\n",
        "  df_headind = Df_filter[Df_filter['block_num'].isin(must_head_blok_list)]\n",
        "\n",
        "\n",
        "\n",
        "  # NAME\n",
        "  # pattern = '|'.join(key_word)\n",
        "  # df_headind_n =df_headind[~df_headind['text'].str.contains(pattern,na=False, case=False)]  \n",
        "  # numaric_w_n = pd.to_numeric(df_headind_n['width'])\n",
        "  # numaric_h_n = pd.to_numeric(df_headind_n['height'])\n",
        "  # df_waigth_n = numaric_w_n * numaric_h_n\n",
        "  # # charector count\n",
        "  # D_text_count = df_headind_n['text'].str.len()\n",
        "  # df_key_waight_all =df_waigth_n / D_text_count  \n",
        "  # name_index = df_key_waight_all.idxmax()  \n",
        "  # # print(name_index)\n",
        "  # block_num = df_headind_n.loc[name_index].block_num\n",
        "  # # print(block_num)\n",
        "  # # find raws based same block_num \n",
        "  # df2 =df_headind_n.loc[df_headind_n[\"block_num\"] == block_num]\n",
        "  # # get text of same raw and mearg as a string.\n",
        "  # ln = df2['text'].values\n",
        "  # print(' '.join(ln))\n",
        "\n",
        "\n",
        "\n",
        "  return df_headind"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUxEN_Pz9-3N"
      },
      "source": [
        "#find head (heading, no heading)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IglUh0nwato6"
      },
      "source": [
        "def details(Df_filter,Df_head):\n",
        "  word = \"EXPERIENCE\"\n",
        "  head_index = Df_head[Df_head['text'].str.contains(word, na=False, case=False)]\n",
        "  # print(head_index)\n",
        "  if head_index.empty == True:\n",
        "    print(word,\" is not in the Heading list.\")\n",
        "    no_heading(Df_filter,Df_head,word)\n",
        "  else:\n",
        "    print(word, \"is in the Heading list.\")\n",
        "    heading(Df_filter,Df_head,word,head_index)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbnGFOVv9o2i"
      },
      "source": [
        "# heading = TRUE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VukqALZKRZD"
      },
      "source": [
        "# heading = TRUE\n",
        "def heading(Df_filter,Df_head,word,head_index):\n",
        "    \n",
        "  # get Details(df) between two heading\n",
        "  start_index = head_index.index.values[0] \n",
        "  # print(start_index)\n",
        "  df_head_index_list = Df_head.index.values.tolist()\n",
        "  end_index = df_head_index_list[(df_head_index_list.index(start_index) + 1) % len(df_head_index_list)] -1\n",
        "  # print(end_index)\n",
        "  df_data_word = Df_filter.loc[start_index+1 : end_index]\n",
        "  # print(df_data_word)\n",
        "\n",
        "  # get number of block count , between two heading(ovvoru blocklayum periya k_waightku uriya word.a eduthu, athu Key_wordku\n",
        "  # samana irukkanu paarthu = enda athodaye stop pannanum, illana adutha head vara vaasikanum)\n",
        "  # get block numbers\n",
        "  blok_tuple = (df_data_word[\"block_num\"].drop_duplicates())\n",
        "  # print(df_data_word[df_data_word[\"block_num\"].isin([17])])\n",
        "  blok_len = len(blok_tuple)\n",
        "  # print(blok_len)\n",
        "  b = 0\n",
        "  while (b < blok_len):\n",
        "    blok_num=blok_tuple.iloc[b]\n",
        "    a_blok_text_set = df_data_word[df_data_word[\"block_num\"].isin([blok_num])]\n",
        "    numaric_w_w = pd.to_numeric(a_blok_text_set['width'])\n",
        "    numaric_h_w = pd.to_numeric(a_blok_text_set['height'])\n",
        "    df_waigth_word = numaric_w_w * numaric_h_w\n",
        "    D_text_count_head = a_blok_text_set['text'].str.len()\n",
        "    df_key_waight_word = df_waigth_word / D_text_count_head\n",
        "    max_key_waight = df_key_waight_word.idxmax()\n",
        "    max_w_word_a_blok = df_data_word[\"text\"].loc[[max_key_waight]]\n",
        "    \n",
        "    key_word = [\"Education\",\"Project\",\"Referees\",\"Skill\",\"Award\",\"Professional\",\"INTEREST\",\n",
        "                \"ACTIVITIES\",\"PUBLICATION\",\"Activity\",\"Hobbies\"]\n",
        "\n",
        "    pattern = '|'.join(key_word)\n",
        "    df_if_in_word = max_w_word_a_blok.str.contains(pattern,na=False, case=False).values[0]\n",
        "    # print(df_if_in_word)\n",
        "    if df_if_in_word == True:\n",
        "      end_index = max_w_word_a_blok  \n",
        "      break  \n",
        "    b += 1\n",
        "\n",
        "  df_data_word_finl = Df_filter.loc[start_index+1 : end_index]\n",
        "  ln = df_data_word_finl['text'].values\n",
        "  details_text = '\\n'.join(' '.join(ln).split('. '))\n",
        "  print(details_text)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa6gdTOp9hWC"
      },
      "source": [
        "# Heading Fales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP7T-yr9QHz0"
      },
      "source": [
        "# Heading Fales \n",
        "# word.a thedi eduthu, athila periya k_waight irukratha select panni\n",
        "# athukku aduthatha ulla head.a thedi eduththu, idaila ullatha df aakkanum, next ,Heading True.la ulla pola seiyanum.\n",
        "\n",
        "def no_heading(Df_filter,Df_head,word):\n",
        "\n",
        "  head_index_tuple = Df_filter[Df_filter['text'].str.contains(word, na=False, case=False)]\n",
        "  # print(head_index_tuple)\n",
        "\n",
        "  if head_index_tuple.empty:\n",
        "    print(\"No word in the cv\")    \n",
        "\n",
        "  else:\n",
        "    numaric_w_w = pd.to_numeric(head_index_tuple['width'])\n",
        "    numaric_h_w = pd.to_numeric(head_index_tuple['height'])\n",
        "    df_waigth_word = numaric_w_w * numaric_h_w\n",
        "    D_text_count_head = head_index_tuple['text'].str.len()\n",
        "    df_key_waight_word = df_waigth_word / D_text_count_head\n",
        "    # print(df_key_waight_word)\n",
        "    max_key_waight = df_key_waight_word.idxmax()\n",
        "    # print(max_key_waight)\n",
        "    max_w_word_a_blok = Df_filter[\"text\"].loc[[max_key_waight]]\n",
        "\n",
        "    big_word_index = max_w_word_a_blok.index[0]\n",
        "    # print(big_word_index)\n",
        "    # find next head in Df_head\n",
        "      # find last head index\n",
        "    # print(Df_head[\"text\"].tolist())\n",
        "    head_index = Df_head.index.tolist() \n",
        "    last_head_index = Df_head.iloc[[-1]].index[0]\n",
        "    print(last_head_index , big_word_index )\n",
        "\n",
        "    if last_head_index <= big_word_index:\n",
        "      df_data_word_bigword = Df_filter.loc[big_word_index+1 :]  \n",
        "      print(\"word in last heading\")\n",
        "\n",
        "    else: \n",
        "      print(\"word in center area\")\n",
        "      next_head_bigword = [i for i in head_index if i > big_word_index][0]\n",
        "      # print(head_index)\n",
        "      print(big_word_index+1 , next_head_bigword-1 )\n",
        "      df_data_word_bigword = Df_filter.loc[big_word_index+1 : next_head_bigword-1]\n",
        "      # print(df_data_word_bigword)\n",
        "\n",
        "    ln = df_data_word_bigword['text'].values\n",
        "    details_text = '\\n'.join(' '.join(ln).split('. '))\n",
        "    print(details_text)  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0kfcgOVcCO-"
      },
      "source": [
        "#DF by OCR - main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9TWITHc7jnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d860f13-494d-4755-857d-8329f2a3bead"
      },
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pdf2image\n",
        "try:\n",
        "    from PIL import Image\n",
        "except ImportError:\n",
        "    import Image\n",
        "import pytesseract\n",
        "\n",
        "\n",
        "def pdf_to_img(pdf_file):\n",
        "    return pdf2image.convert_from_path('/content/drive/MyDrive/Example/Dinuka_Rumesh_SE.pdf')\n",
        "\n",
        "\n",
        "def ocr_core(file):\n",
        "    df_text = pytesseract.image_to_data(file)\n",
        "    return df_text\n",
        "\n",
        "def head_find(Df_filter1):\n",
        "    dff1 = Df_filter1\n",
        "    Df_head_e = pd.DataFrame([]) \n",
        "    df_head1=find_head(dff1) \n",
        "    Df_head_e = Df_head_e.append(df_head1)\n",
        "    return Df_head_e\n",
        "\n",
        "\n",
        "def print_pages(pdf_file):\n",
        "    images = pdf_to_img(pdf_file)\n",
        "    Df_filter_e = pd.DataFrame([])\n",
        "    # Df_head = pd.DataFrame([])\n",
        "    for pg, img in enumerate(images):\n",
        "        df_img = ocr_core(img)\n",
        "        # dff = save_df(df_img)\n",
        "        if Df_filter_e.empty == True:\n",
        "          df_fill1,next_first_blok=save_df(df_img,0) \n",
        "          Df_filter_e = Df_filter_e.append(df_fill1,ignore_index=True)\n",
        "        else:\n",
        "          last_blok = Df_filter_e[\"block_num\"].iloc[-1]+1\n",
        "          blok_add_no = last_blok - next_first_blok\n",
        "          df_fill1,next_first_blok=save_df(df_img,blok_add_no) \n",
        "          Df_filter_e = Df_filter_e.append(df_fill1,ignore_index=True)\n",
        "        \n",
        "    \n",
        "    Df_head = head_find(Df_filter_e)\n",
        "    details(Df_filter_e,Df_head)\n",
        "    print(Df_head)\n",
        "    # print(Df_filter_e)\n",
        "\n",
        "    # Df_head.to_csv (r'/content/head.csv', index = False, header=True)\n",
        "    # Df_filter_e.to_csv (r'/content/data1.csv', index = False, header=True) \n",
        "       \n",
        "print_pages('sample.pdf')"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EXPERIENCE  is not in the Heading list.\n",
            "252 76\n",
            "word in center area\n",
            "77 251\n",
            "in Software Development and Testing\n",
            "Furthermore, | am adept at handling multiple tasks on a daily basis competently and at working well under pressure\n",
            "A key strength is communication; building strong relationships with people in order to deliver the best results\n",
            "UNIVERSITY OF MORATUWA: FACULTY OF INFORMATION TECHNOLOGY Reading for B.Sc\n",
            "(Hons.) Degree in Information Technology\n",
            "(Expected 2019) SIVALI CENTRAL COLLAGE, RATHNAPURA\n",
            "G.C.E Advanced Level - Physical Science Stream (2014) Results: Chemistry - B, Combined Mathematics - B, Physics - B SIVALI CENTRAL COLLAGE, RATHNAPURA G.C.E Ordinary Level (2010) Results: 7 A's 2 B's MOBITEL ENGINEERING DIVISION | 2018 September - 2019 February Intern Software Engineering Experience: Engineering Asset Management Portal It is a resource management system which is developed with the intention of monitoring the resources issued by the seven divisions at Mobitel\n",
            "Technologies Used: PHP CO -FOUNDER | 2018 February- 2019 May Team Vicerent | have worked as a project manager at Team VICERANT in a web-based affiliate marketing project called GIFT CABIN\n",
            "VOLUNTEERING Conduct outreach programs for rural schools by Team Vicerant\n",
            "    level page_num block_n par_num  ... height conf                 text block_num\n",
            "0       5        1       1       1  ...     33   96              CONTACT       1.0\n",
            "19      5        1       7       1  ...     33   96               SKILLS       7.0\n",
            "252     5        1       1       1  ...     34   91  ACTIVITIES/INTEREST      29.0\n",
            "\n",
            "[3 rows x 13 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muJQHVHAH-u-"
      },
      "source": [
        "# find Details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvWWjU_9IVu_"
      },
      "source": [
        "a, b = print_pages('sample.pdf')\n",
        "Df_head = a\n",
        "Df_filter = b\n",
        "# print(df_head)\n",
        "# print(Df_filter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6qAo7q9cNlP"
      },
      "source": [
        "#moving avg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arDDFvo1sznW"
      },
      "source": [
        "# df_last_values = one_or_two_blok_heading[\"key waigth\"]\n",
        "# print(df_last_values)\n",
        "# Data_point_m= df_last_values.rolling(2, min_periods=1).mean()\n",
        "# Data_point_m_D = Data_point_m.squeeze().tolist()\n",
        "# round_to_whole = [round(num) for num in Data_point_m_D]\n",
        "# print(round_to_whole)\n",
        "# may_reduse=((sum(round_to_whole) / len(round_to_whole)) *0.25)\n",
        "# moving_avg = ((sum(round_to_whole) / len(round_to_whole)))\n",
        "# may_lower = moving_avg - may_reduse\n",
        "# if (may_lower < min(df_last_values)):\n",
        "#   avg_key_waight = min(df_last_values)\n",
        "# else:\n",
        "#   avg_key_waight = min(round_to_whole)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_ukkzoyfVaj"
      },
      "source": [
        "#org"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsMLk_pS-uj1"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import sys\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "import collections\n",
        "\n",
        "# Path of the pdf\n",
        "PDF_file = \"/content/drive/MyDrive/Example/Alagaretnam Elakyaveena.pdf\"\n",
        "\n",
        "'''\n",
        "Part #1 : Converting PDF to images\n",
        "'''\n",
        "\n",
        "# Store all the pages of the PDF in a variable\n",
        "pages = convert_from_path(PDF_file, 500)\n",
        "\n",
        "# Counter to store images of each page of PDF to image\n",
        "image_counter = 1\n",
        "\n",
        "# Iterate through all the pages stored above\n",
        "for page in pages:\n",
        "\n",
        "\t# Declaring filename for each page of PDF as JPG\n",
        "\t# For each page, filename will be:\n",
        "\t# PDF page 1 -> page_1.jpg\n",
        "\t# PDF page 2 -> page_2.jpg\n",
        "\t# PDF page 3 -> page_3.jpg\n",
        "\t# ....\n",
        "\t# PDF page n -> page_n.jpg\n",
        "\tfilename = \"page_\"+str(image_counter)+\".jpg\"\n",
        "\t\n",
        "\t# Save the image of the page in system\n",
        "\tpage.save(filename, 'JPEG')\n",
        "\n",
        "\t# Increment the counter to update filename\n",
        "\timage_counter = image_counter + 1\n",
        "\n",
        "'''\n",
        "Part #2 - Recognizing text from the images using OCR\n",
        "'''\n",
        "\t# 3\n",
        "# Variable to get count of total number of pages\n",
        "filelimit = image_counter-1\n",
        "\n",
        "# Creating a text file to write the output\n",
        "outfile = \"out_text.txt\"\n",
        "\n",
        "# Open the file in append mode so that\n",
        "# All contents of all images are added to the same file\n",
        "f = open(outfile, \"a\")\n",
        "\n",
        "# Iterate from 1 to total number of pages\n",
        "for i in range(1, filelimit + 1):\n",
        "\n",
        "\t# Set filename to recognize text from\n",
        "\t# Again, these files will be:\n",
        "\t# page_1.jpg\n",
        "\t# page_2.jpg\n",
        "\t# ....\n",
        "\t# page_n.jpg\n",
        "\tfilename = \"page_\"+str(i)+\".jpg\"\n",
        "\t\t\n",
        "\t# Recognize the text as string in image using pytesserct\n",
        "\ttext = str(((pytesseract.image_to_string(Image.open(filename)))))\n",
        "\n",
        "\t# The recognized text is stored in variable text\n",
        "\t# Any string processing may be applied on text\n",
        "\t# Here, basic formatting has been done:\n",
        "\t# In many PDFs, at line ending, if a word can't\n",
        "\t# be written fully, a 'hyphen' is added.\n",
        "\t# The rest of the word is written in the next line\n",
        "\t# Eg: This is a sample text this word here GeeksF-\n",
        "\t# orGeeks is half on first line, remaining on next.\n",
        "\t# To remove this, we replace every '-\\n' to ''.\n",
        "\ttext = text.replace('-\\n', '')\t\n",
        "\n",
        "\t# Finally, write the processed text to the file.\n",
        "\tf.write(text)\n",
        "\n",
        "# Close the file after writing all the text.\n",
        "f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PibfYydrYD8Q"
      },
      "source": [
        "#name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKRTaB1wsffe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4b5333-a4d5-4460-eb06-7e32e23f6016"
      },
      "source": [
        "# get data from image\n",
        "table_Cv = pytesseract.image_to_data(Image.open(filename))\n",
        "# image data to DF\n",
        "table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "# print(table_Cv_df)\n",
        "# table_Cv = pytesseract.image_to_data(Image.open(filename))\n",
        "# image data to DF\n",
        "# table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "# add headera same as 1st row\n",
        "table_Cv_df1 = table_Cv_df.rename(columns=table_Cv_df.iloc[0])\n",
        "# remove 1st row\n",
        "table_Cv_df1  = table_Cv_df1[1:]\n",
        "\n",
        "# filter DF (empty text, left and top 0, remove \"e\":line)\n",
        "Df_filter = table_Cv_df1.loc[(table_Cv_df1.text != ' ') & (table_Cv_df1.left != '0') & \n",
        "                             (table_Cv_df1.top != '0')& (table_Cv_df1.text != 'e')& (table_Cv_df1.text != \"\")]\n",
        "name = (Df_filter.width) \n",
        "Df_filter.to_csv (r'/content/amilD.csv', index = False, header=True)\n",
        "# get high widht size as it is a NAME\n",
        "big_word =(max(Df_filter.width.astype(float)))\n",
        "# get the big size word block number\n",
        "block_num = Df_filter[\"block_num\"].iloc[Df_filter[\"width\"].astype(float).argmax()]\n",
        "# find raws based same block_num \n",
        "df2 =Df_filter.loc[Df_filter[\"block_num\"] == block_num]\n",
        "# get text of same raw and mearg as a string.\n",
        "ln = df2['text'].values\n",
        "print(' '.join(ln))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AWARDS & ACHIEVEMENTS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRcgO7_cQ-oy"
      },
      "source": [
        "#word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk8WyisNxveS",
        "outputId": "f72e419a-6852-4488-f239-5b6aad022342"
      },
      "source": [
        "#find the word, get that row tuple\n",
        "word = \"Experience\"\n",
        "# word = \"Education\"\n",
        "# find word tuple\n",
        "df_tuple_word=Df_filter[Df_filter['text'].str.contains(word, na=False, case=False)]\n",
        "# index of the word in original DF and next index of filter df////////////////\n",
        "# array of filtred df index\n",
        "place_num =Df_filter.index.values\n",
        "# index of original df\n",
        "word_index = df_tuple_word.index.values[0]\n",
        "# original index number on array\n",
        "x = np.where(place_num == word_index)\n",
        "# next to filterd df index of word in array\n",
        "index_new = x[0][0] + 1\n",
        "# index of original df\n",
        "next_indx = place_num[index_new]\n",
        "# block number of next to word bloc number\n",
        "next_blok_num = Df_filter.block_num[next_indx]\n",
        "# print(next_blok_num)\n",
        "# if heading and details in same block (check next raw is in same with word block number)\n",
        "if (int(next_blok_num) == int(df_tuple_word['block_num'])):\n",
        "  df3_b_num = int(df_tuple_word['block_num']) \n",
        "  \n",
        "else:\n",
        "  # sum one to find next raw num (experiance details in start from next line of experiance)\n",
        "  df3_b_num = int(df_tuple_word['block_num']) + 1\n",
        "# get df that: experiance data in same block number\n",
        "df_same_bloc_num =df_tuple_word.loc[df_tuple_word[\"block_num\"] == str(df3_b_num)]\n",
        "print(df_same_bloc_num)\n",
        "# get totel line of expariance\n",
        "# print(df_same_bloc_num.par_num.astype(float))\n",
        "try:\n",
        "  max_line_num = max(df_same_bloc_num.par_num.astype(float))\n",
        "except:\n",
        "  max_line_num = 1\n",
        "\n",
        "# loop for get line by line basd on par_num\n",
        "p_n = 1\n",
        "print(max_line_num)\n",
        "while (p_n <= max_line_num):\n",
        "  # creat df : for one line (filter based same par_num)\n",
        "  df_par_num =df_same_bloc_num.loc[df_same_bloc_num[\"par_num\"] == str(p_n)]\n",
        "  # add word of same par_num\n",
        "  ln2 = df_par_num['text'].values\n",
        "  word_set = ' '.join(ln2)\n",
        "  p_n =p_n+1;  \n",
        "print(word_set)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-6fab16a4cff7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplace_num\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mDf_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# index of original df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_tuple_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# original index number on array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplace_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE_h0egGOP0g"
      },
      "source": [
        "#findHead Algo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hldDUn3LEBHp"
      },
      "source": [
        "#save df1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUqphpCeyqnS"
      },
      "source": [
        "table_Cv = pytesseract.image_to_data(Image.open(filename))\n",
        "# print(table_Cv.shape)\n",
        "table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "print(table_Cv_df)\n",
        "table_Cv_df.to_csv (r'/content/export_dataframe.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}