{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arimac_by_me.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZM78AGPYNi0g",
        "S_ukkzoyfVaj",
        "PibfYydrYD8Q",
        "zRcgO7_cQ-oy"
      ],
      "mount_file_id": "1tSh4xmTafMD88IbBM5rDQwoVr3GdvS5q",
      "authorship_tag": "ABX9TyMPczoXN6KG/cPdducwoffY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VEENA9/Leaf-/blob/master/Arimac_by_me.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kzfto-ihdPn"
      },
      "source": [
        "#lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOigv-x_b3wI"
      },
      "source": [
        "!pip3 install Pillow\n",
        "!pip3 install pytesseract\n",
        "!pip3 install pdf2image\n",
        "!sudo apt-get install tesseract-ocr\n",
        "!apt-get install poppler-utils \n",
        "!pip install python-poppler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM78AGPYNi0g"
      },
      "source": [
        "#Filtred DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOfOnBjBDbTX"
      },
      "source": [
        "# get data from image\n",
        "def save_df(df_img):\n",
        "\n",
        "  table_Cv = df_img\n",
        "  # image data to DF\n",
        "  table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "  # image data to DF\n",
        "  # add headera same as 1st row\n",
        "  table_Cv_df1 = table_Cv_df.rename(columns=table_Cv_df.iloc[0])\n",
        "  # remove 1st row\n",
        "  table_Cv_df1  = table_Cv_df1[1:]\n",
        "\n",
        "  # filter DF (empty text, left and top 0, remove \"e\":line)\n",
        "  Df_filter1 = table_Cv_df1.loc[(table_Cv_df1.text != ' ') & (table_Cv_df1.left != '0') & \n",
        "                              (table_Cv_df1.top != '0')& (table_Cv_df1.text != 'e')& (table_Cv_df1.text != \"\")]\n",
        "\n",
        "  # print(Df_filter) \n",
        "  return Df_filter1                             "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uU6WTrVOCQM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olVgQn_IOCpj"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6purK4nNbTZ"
      },
      "source": [
        "#DF by OCR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9TWITHc7jnY"
      },
      "source": [
        "import collections\n",
        "import pandas as pd\n",
        "import pdf2image\n",
        "try:\n",
        "    from PIL import Image\n",
        "except ImportError:\n",
        "    import Image\n",
        "import pytesseract\n",
        "\n",
        "\n",
        "def pdf_to_img(pdf_file):\n",
        "    return pdf2image.convert_from_path('/content/drive/MyDrive/Example/Alagaretnam Elakyaveena.pdf')\n",
        "\n",
        "\n",
        "def ocr_core(file):\n",
        "    df_text = pytesseract.image_to_data(file)\n",
        "    return df_text\n",
        "\n",
        "\n",
        "def print_pages(pdf_file):\n",
        "    images = pdf_to_img(pdf_file)\n",
        "    Df_filter = pd.DataFrame([])\n",
        "    for pg, img in enumerate(images):\n",
        "        df_img = ocr_core(img)\n",
        "        dff = save_df(df_img)\n",
        "        df_head=find_head(dff)\n",
        "        Df_filter = Df_filter.append(dff)\n",
        "    print(dff)\n",
        "    return dff\n",
        "    # df.to_csv (r'/content/df.csv', index = False, header=True)\n",
        "        \n",
        "\n",
        "\n",
        "print_pages('sample.pdf')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_ukkzoyfVaj"
      },
      "source": [
        "#org"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsMLk_pS-uj1"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import sys\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "import collections\n",
        "\n",
        "# Path of the pdf\n",
        "PDF_file = \"/content/drive/MyDrive/Example/Chalani_Aththanayake_QA .pdf\"\n",
        "\n",
        "'''\n",
        "Part #1 : Converting PDF to images\n",
        "'''\n",
        "\n",
        "# Store all the pages of the PDF in a variable\n",
        "pages = convert_from_path(PDF_file, 500)\n",
        "\n",
        "# Counter to store images of each page of PDF to image\n",
        "image_counter = 1\n",
        "\n",
        "# Iterate through all the pages stored above\n",
        "for page in pages:\n",
        "\n",
        "\t# Declaring filename for each page of PDF as JPG\n",
        "\t# For each page, filename will be:\n",
        "\t# PDF page 1 -> page_1.jpg\n",
        "\t# PDF page 2 -> page_2.jpg\n",
        "\t# PDF page 3 -> page_3.jpg\n",
        "\t# ....\n",
        "\t# PDF page n -> page_n.jpg\n",
        "\tfilename = \"page_\"+str(image_counter)+\".jpg\"\n",
        "\t\n",
        "\t# Save the image of the page in system\n",
        "\tpage.save(filename, 'JPEG')\n",
        "\n",
        "\t# Increment the counter to update filename\n",
        "\timage_counter = image_counter + 1\n",
        "\n",
        "'''\n",
        "Part #2 - Recognizing text from the images using OCR\n",
        "'''\n",
        "\t# 3\n",
        "# Variable to get count of total number of pages\n",
        "filelimit = image_counter-1\n",
        "\n",
        "# Creating a text file to write the output\n",
        "outfile = \"out_text.txt\"\n",
        "\n",
        "# Open the file in append mode so that\n",
        "# All contents of all images are added to the same file\n",
        "f = open(outfile, \"a\")\n",
        "\n",
        "# Iterate from 1 to total number of pages\n",
        "for i in range(1, filelimit + 1):\n",
        "\n",
        "\t# Set filename to recognize text from\n",
        "\t# Again, these files will be:\n",
        "\t# page_1.jpg\n",
        "\t# page_2.jpg\n",
        "\t# ....\n",
        "\t# page_n.jpg\n",
        "\tfilename = \"page_\"+str(i)+\".jpg\"\n",
        "\t\t\n",
        "\t# Recognize the text as string in image using pytesserct\n",
        "\ttext = str(((pytesseract.image_to_string(Image.open(filename)))))\n",
        "\n",
        "\t# The recognized text is stored in variable text\n",
        "\t# Any string processing may be applied on text\n",
        "\t# Here, basic formatting has been done:\n",
        "\t# In many PDFs, at line ending, if a word can't\n",
        "\t# be written fully, a 'hyphen' is added.\n",
        "\t# The rest of the word is written in the next line\n",
        "\t# Eg: This is a sample text this word here GeeksF-\n",
        "\t# orGeeks is half on first line, remaining on next.\n",
        "\t# To remove this, we replace every '-\\n' to ''.\n",
        "\ttext = text.replace('-\\n', '')\t\n",
        "\n",
        "\t# Finally, write the processed text to the file.\n",
        "\tf.write(text)\n",
        "\n",
        "# Close the file after writing all the text.\n",
        "f.close()\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PibfYydrYD8Q"
      },
      "source": [
        "#name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKRTaB1wsffe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd33722-36d8-4403-d618-1c6b8744c80e"
      },
      "source": [
        "# get data from image\n",
        "table_Cv = pytesseract.image_to_data(Image.open(filename))\n",
        "# image data to DF\n",
        "table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "# print(table_Cv_df)\n",
        "# table_Cv = pytesseract.image_to_data(Image.open(filename))\n",
        "# image data to DF\n",
        "# table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "# add headera same as 1st row\n",
        "table_Cv_df1 = table_Cv_df.rename(columns=table_Cv_df.iloc[0])\n",
        "# remove 1st row\n",
        "table_Cv_df1  = table_Cv_df1[1:]\n",
        "\n",
        "# filter DF (empty text, left and top 0, remove \"e\":line)\n",
        "Df_filter = table_Cv_df1.loc[(table_Cv_df1.text != ' ') & (table_Cv_df1.left != '0') & \n",
        "                             (table_Cv_df1.top != '0')& (table_Cv_df1.text != 'e')& (table_Cv_df1.text != \"\")]\n",
        "name = (Df_filter.width) \n",
        "Df_filter.to_csv (r'/content/amilD.csv', index = False, header=True)\n",
        "# get high widht size as it is a NAME\n",
        "big_word =(max(Df_filter.width.astype(float)))\n",
        "# get the big size word block number\n",
        "block_num = Df_filter[\"block_num\"].iloc[Df_filter[\"width\"].astype(float).argmax()]\n",
        "# find raws based same block_num \n",
        "df2 =Df_filter.loc[Df_filter[\"block_num\"] == block_num]\n",
        "# get text of same raw and mearg as a string.\n",
        "ln = df2['text'].values\n",
        "print(' '.join(ln))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ah Od Jf Internship — Atlink Communication Pvt Ltd (From August\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRcgO7_cQ-oy"
      },
      "source": [
        "#word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk8WyisNxveS",
        "outputId": "bfe52347-d41b-4f5b-baa4-bd51dd5b4d27"
      },
      "source": [
        "#find the word, get that row tuple\n",
        "# word = \"Experience\"\n",
        "word = \"Education\"\n",
        "# find word tuple\n",
        "df_tuple_word=Df_filter[Df_filter['text'].str.contains(word, na=False, case=False)]\n",
        "# index of the word in original DF and next index of filter df////////////////\n",
        "# array of filtred df index\n",
        "place_num =Df_filter.index.values\n",
        "# index of original df\n",
        "word_index = df_tuple_word.index.values[0]\n",
        "# original index number on array\n",
        "x = np.where(place_num == word_index)\n",
        "# next to filterd df index of word in array\n",
        "index_new = x[0][0] + 1\n",
        "# index of original df\n",
        "next_indx = place_num[index_new]\n",
        "# block number of next to word bloc number\n",
        "next_blok_num = Df_filter.block_num[next_indx]\n",
        "# print(next_blok_num)\n",
        "# if heading and details in same block (check next raw is in same with word block number)\n",
        "if (int(next_blok_num) == int(df_tuple_word['block_num'])):\n",
        "  df3_b_num = int(df_tuple_word['block_num']) \n",
        "  \n",
        "else:\n",
        "  # sum one to find next raw num (experiance details in start from next line of experiance)\n",
        "  df3_b_num = int(df_tuple_word['block_num']) + 1\n",
        "# get df that: experiance data in same block number\n",
        "df_same_bloc_num =df_tuple_word.loc[df_tuple_word[\"block_num\"] == str(df3_b_num)]\n",
        "print(df_same_bloc_num)\n",
        "# get totel line of expariance\n",
        "# print(df_same_bloc_num.par_num.astype(float))\n",
        "try:\n",
        "  max_line_num = max(df_same_bloc_num.par_num.astype(float))\n",
        "except:\n",
        "  max_line_num = 1\n",
        "\n",
        "# loop for get line by line basd on par_num\n",
        "p_n = 1\n",
        "print(max_line_num)\n",
        "while (p_n <= max_line_num):\n",
        "  # creat df : for one line (filter based same par_num)\n",
        "  df_par_num =df_same_bloc_num.loc[df_same_bloc_num[\"par_num\"] == str(p_n)]\n",
        "  # add word of same par_num\n",
        "  ln2 = df_par_num['text'].values\n",
        "  print(' '.join(ln2))\n",
        "  p_n =p_n+1;  "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   level page_num block_num par_num  ... width height conf         text\n",
            "20     5        1         2       2  ...   349     46   86  Educational\n",
            "\n",
            "[1 rows x 12 columns]\n",
            "2.0\n",
            "\n",
            "Educational\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE_h0egGOP0g"
      },
      "source": [
        "#findHead Algo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22sUwHJalNEl"
      },
      "source": [
        "Df_filter = print_pages('sample.pdf')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZXcO8eTmWd4"
      },
      "source": [
        "print(Df_filter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnI__1_EOsM6",
        "outputId": "b6496b49-ecce-4027-c7dc-4ccecd024ffb"
      },
      "source": [
        "def find_head():\n",
        "  # Find key heading\n",
        "  key_word = [\"Contact\",\"Education\",\"Project\",\"Experience\",\"Referees\",\"Skill\",\"About\",\"Award\",\"Professional\",\"INTEREST\",\"ACTIVITIES\"]\n",
        "  awoid_key = [\"www\",\"@\",\"com\",\"/\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\"]\n",
        "  # get guessed heading  \n",
        "  pattern = '|'.join(key_word)\n",
        "  guessed_heading_df =Df_filter[Df_filter['text'].str.contains(pattern,na=False, case=False)]\n",
        "  # print(guessed_heading_df)\n",
        "  # get the guessed heading block number\n",
        "  block_num_h = guessed_heading_df[\"block_num\"].values.tolist()\n",
        "  # print(block_num_h)\n",
        "  # find raws based same block_num \n",
        "  full_heading_df = Df_filter[Df_filter['block_num'].isin(block_num_h)]\n",
        "  # print(full_heading_df['block_num'].values.tolist())\n",
        "  # block number from original df\n",
        "  same_block_no=full_heading_df['block_num'].values.tolist()\n",
        "  # print(same_block_no)\n",
        "  # 1 or 2 blocks items (we assume heading must on 1 or 2 words) (get 1 or 2 block word from originl df)\n",
        "  must_head_blok_list = [item for item, count in collections.Counter(same_block_no).items() if (count <= 3)]\n",
        "  # print(must_head_blok_list)\n",
        "  # find df of must head \n",
        "  one_or_two_blok_heading = Df_filter[Df_filter['block_num'].isin(must_head_blok_list)]\n",
        "  # print(one_or_two_blok_heading)\n",
        "\n",
        "  # key waigth////// in one_or_two_blok_heading \n",
        "  one_or_two_blok_heading_w = pd.to_numeric(one_or_two_blok_heading['width'])\n",
        "  one_or_two_blok_heading_h = pd.to_numeric(one_or_two_blok_heading['height'])\n",
        "  # print(one_or_two_blok_heading_h.values.tolist())\n",
        "  df_waigth = one_or_two_blok_heading_w * one_or_two_blok_heading_h\n",
        "  # print(df_waigth.values.tolist())\n",
        "  D_text_count = one_or_two_blok_heading['text'].str.len()\n",
        "  # print(D_text_count.values.tolist())\n",
        "  df_key_waight =df_waigth / D_text_count\n",
        "  # print(df_key_waight)\n",
        "\n",
        "  avg_key_waight = df_key_waight.min(skipna=True)\n",
        "  # print(avg_key_waight)\n",
        "\n",
        "  # ///////////////\n",
        "  Df_filter_w = pd.to_numeric(Df_filter['width'])\n",
        "  Df_filter_h = pd.to_numeric(Df_filter['height'])\n",
        "  df_waigth_all = Df_filter_w * Df_filter_h\n",
        "  # print(df_waigth_all.values.tolist())\n",
        "  D_text_count_all = Df_filter['text'].str.len()\n",
        "  df_key_waight_all =df_waigth_all / D_text_count_all\n",
        "  # print(df_key_waight_all)\n",
        "  # ///////////////\n",
        "\n",
        "  # print(Df_filter[Df_filter['key waigth'] > 25860 ])\n",
        "  Df_waight_filter = df_key_waight_all[df_key_waight_all > avg_key_waight] \n",
        "  print(Df_waight_filter)\n",
        "\n",
        "  Df_waight_filter_text = pd.concat([Df_waight_filter, Df_filter.reindex(Df_waight_filter.index)], axis=1)\n",
        "  print(Df_waight_filter_text)\n",
        "  # print(Df_waight_filter_text)\n",
        "\n",
        "  # awoid keywords\n",
        "  awoid_pattern = '|'.join(awoid_key)\n",
        "  awoiding_heading_df =Df_waight_filter_text[~Df_waight_filter_text['text'].str.contains(awoid_pattern,na=False, case=False)]\n",
        "  print(awoiding_heading_df)\n",
        "\n",
        "  # Df_filter.to_csv (r'/content/amilchal.csv', index = False, header=True)\n",
        "  return awoiding_heading_df"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    level page_num block_num par_num line_num  ...   top width height conf      text\n",
            "10      5        1         2       1        1  ...   345   108     28   96     SKILL\n",
            "72      5        1         9       1        1  ...  1063   102     33   96   Project\n",
            "110     5        1        13       1        1  ...  1511   187     28   96  REFEREES\n",
            "224     5        1        17       6        1  ...   698   106     28   96   project\n",
            "331     5        1        18       3        1  ...  1276   106     28   96   project\n",
            "372     5        1        19       1        1  ...  1593   139     27   93    AWARDS\n",
            "381     5        1        20       1        1  ...  1688    87     24   96     Award\n",
            "438     5        1        20       5        1  ...  1999    87     24   96     Award\n",
            "\n",
            "[8 rows x 12 columns]\n",
            "['2', '9', '13', '17', '18', '19', '20', '20']\n",
            "['2', '2', '9', '9', '9', '9', '13', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '18', '19', '19', '19', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20', '20']\n",
            "['2', '13', '19']\n",
            "    level page_num block_num par_num  ... width height conf          text\n",
            "9       5        1         2       1  ...   218     28   94     TECHNICAL\n",
            "10      5        1         2       1  ...   108     28   96         SKILL\n",
            "110     5        1        13       1  ...   187     28   96      REFEREES\n",
            "372     5        1        19       1  ...   139     27   93        AWARDS\n",
            "373     5        1        19       1  ...    58     27   92             &\n",
            "374     5        1        19       1  ...   301     27   95  ACHIEVEMENTS\n",
            "\n",
            "[6 rows x 12 columns]\n",
            "9       678.222222\n",
            "10      604.800000\n",
            "110     654.500000\n",
            "372     625.500000\n",
            "373    1566.000000\n",
            "374     677.250000\n",
            "dtype: float64\n",
            "604.8\n",
            "9       678.222222\n",
            "110     654.500000\n",
            "171     803.250000\n",
            "172     648.000000\n",
            "372     625.500000\n",
            "373    1566.000000\n",
            "374     677.250000\n",
            "dtype: float64\n",
            "               0 level page_num block_num  ... width height conf          text\n",
            "9     678.222222     5        1         2  ...   218     28   94     TECHNICAL\n",
            "110   654.500000     5        1        13  ...   187     28   96      REFEREES\n",
            "171   803.250000     5        1        16  ...   119     27   96          WORK\n",
            "172   648.000000     5        1        16  ...   240     27   96    EXPERIANCE\n",
            "372   625.500000     5        1        19  ...   139     27   93        AWARDS\n",
            "373  1566.000000     5        1        19  ...    58     27   92             &\n",
            "374   677.250000     5        1        19  ...   301     27   95  ACHIEVEMENTS\n",
            "\n",
            "[7 rows x 13 columns]\n",
            "               0 level page_num block_num  ... width height conf          text\n",
            "9     678.222222     5        1         2  ...   218     28   94     TECHNICAL\n",
            "110   654.500000     5        1        13  ...   187     28   96      REFEREES\n",
            "171   803.250000     5        1        16  ...   119     27   96          WORK\n",
            "172   648.000000     5        1        16  ...   240     27   96    EXPERIANCE\n",
            "372   625.500000     5        1        19  ...   139     27   93        AWARDS\n",
            "373  1566.000000     5        1        19  ...    58     27   92             &\n",
            "374   677.250000     5        1        19  ...   301     27   95  ACHIEVEMENTS\n",
            "\n",
            "[7 rows x 13 columns]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arDDFvo1sznW"
      },
      "source": [
        "df_last_values = one_or_two_blok_heading[\"key waigth\"]\n",
        "print(df_last_values)\n",
        "Data_point_m= df_last_values.rolling(2, min_periods=1).mean()\n",
        "Data_point_m_D = Data_point_m.squeeze().tolist()\n",
        "round_to_whole = [round(num) for num in Data_point_m_D]\n",
        "print(round_to_whole)\n",
        "may_reduse=((sum(round_to_whole) / len(round_to_whole)) *0.25)\n",
        "moving_avg = ((sum(round_to_whole) / len(round_to_whole)))\n",
        "may_lower = moving_avg - may_reduse\n",
        "if (may_lower < min(df_last_values)):\n",
        "  avg_key_waight = min(df_last_values)\n",
        "else:\n",
        "  avg_key_waight = min(round_to_whole)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzNDWA5tuapi"
      },
      "source": [
        "df = pd.DataFrame({'A': [5,6,3,4], 'B': [1,2,3,5]})\n",
        "print(df['A'])\n",
        "print(df[df['A'].isin([3, 6])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWkeZOUcTi3o"
      },
      "source": [
        "print(Df_filter[Df_filter['text'].str.contains(\"EDUCATION\",na=False, case=False)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hldDUn3LEBHp"
      },
      "source": [
        "#save df1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUqphpCeyqnS"
      },
      "source": [
        "table_Cv = pytesseract.image_to_data(Image.open(filename))\n",
        "# print(table_Cv.shape)\n",
        "table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "print(table_Cv_df)\n",
        "table_Cv_df.to_csv (r'/content/export_dataframe.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ji9Blw_KcpJ"
      },
      "source": [
        "print(pytesseract.image_to_data(Image.open(filename)))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}