{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3Kzfto-ihdPn",
        "ZM78AGPYNi0g",
        "u2zQBkCTKZbM",
        "pUxEN_Pz9-3N",
        "lbnGFOVv9o2i",
        "wa6gdTOp9hWC",
        "yrdr9s3M6dzy",
        "w0kfcgOVcCO-"
      ],
      "toc_visible": true,
      "mount_file_id": "14QTdRuSlNk356Z3qoV0UDiOQtfgACML-",
      "authorship_tag": "ABX9TyNX5FYhx/QyvNZY9+cfoeUk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VEENA9/Leaf-/blob/master/E_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kzfto-ihdPn"
      },
      "source": [
        "#lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOigv-x_b3wI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b50ae1-6dae-425d-83b4-88b64a2eb313"
      },
      "source": [
        "!pip3 install Pillow\n",
        "!pip3 install pytesseract\n",
        "!pip3 install pdf2image\n",
        "!sudo apt-get install tesseract-ocr\n",
        "!apt-get install poppler-utils \n",
        "!pip install python-poppler\n",
        "!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Collecting pytesseract\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/e6/a4e9fc8a93c1318540e8de6d8d4beb5749b7960388a7c7f27799fc2dd016/pytesseract-0.3.7.tar.gz\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pytesseract) (7.1.2)\n",
            "Building wheels for collected packages: pytesseract\n",
            "  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytesseract: filename=pytesseract-0.3.7-py2.py3-none-any.whl size=13945 sha256=45a7a0cb1526791172ea8413c6be958690c9734f37a64bffc20aed1960606fbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/20/7e/1dd0daad1575d5260916bb1e9781246430647adaef4b3ca3b3\n",
            "Successfully built pytesseract\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.7\n",
            "Collecting pdf2image\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/17/debc231f9bdb499e93389bb8679c7091ee9e4993dd92ed2da18aa896e2b6/pdf2image-1.15.1-py3-none-any.whl\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pdf2image) (7.1.2)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.15.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 4,795 kB of archives.\n",
            "After this operation, 15.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB]\n",
            "Fetched 4,795 kB in 1s (4,223 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 160706 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 154 kB of archives.\n",
            "After this operation, 613 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 poppler-utils amd64 0.62.0-2ubuntu2.12 [154 kB]\n",
            "Fetched 154 kB in 1s (268 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 160753 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_0.62.0-2ubuntu2.12_amd64.deb ...\n",
            "Unpacking poppler-utils (0.62.0-2ubuntu2.12) ...\n",
            "Setting up poppler-utils (0.62.0-2ubuntu2.12) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting python-poppler\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/77/8f7b5c8ff2c0a7c3afbb3dc8d2342ef2e3ef48053a467e02913e18b73dc6/python-poppler-0.2.2.tar.gz (595kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 5.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: python-poppler\n",
            "  Building wheel for python-poppler (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for python-poppler\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for python-poppler\n",
            "Failed to build python-poppler\n",
            "Installing collected packages: python-poppler\n",
            "    Running setup.py install for python-poppler ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-9ggop63p/python-poppler/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-9ggop63p/python-poppler/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-4cd840jx/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxaFyq5zQYFU"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('templates'):\n",
        "  os.makedirs('templates')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM78AGPYNi0g"
      },
      "source": [
        "#Filtred DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOfOnBjBDbTX"
      },
      "source": [
        "# get data from image\n",
        "def save_df(df_img,last_blok):\n",
        "\n",
        "  table_Cv = df_img\n",
        "  # image data to DF\n",
        "  table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "  # image data to DF\n",
        "  # add headera same as 1st row\n",
        "  table_Cv_df1 = table_Cv_df.rename(columns=table_Cv_df.iloc[0])\n",
        "  # remove 1st row\n",
        "  table_Cv_df1  = table_Cv_df1[1:]\n",
        "\n",
        "  # filter DF (empty text, left and top 0, remove \"e\":line)\n",
        "  Df_filter_f = table_Cv_df1.loc[(table_Cv_df1.text != ' ') & (table_Cv_df1.left != '0')&(table_Cv_df1.text !='  ')&\n",
        "                              (table_Cv_df1.top != '0')& (table_Cv_df1.text != 'e')& (table_Cv_df1.text != \"\")& \n",
        "                              (table_Cv_df1.conf != \"0\"),['block_num','par_num','width','height',\"conf\",'text']]\n",
        "  \n",
        "  # Df_filter1 = table_Cv_df1.loc[(table_Cv_df1.text.str.len() > 3) & (pd.to_numeric(table_Cv_df1['conf']) > 74)]\n",
        "  # print(Df_filter_f)\n",
        "  # (Df_filter_f.text.str.len() > 3)\n",
        "  Df_f_1 = Df_filter_f[pd.to_numeric(Df_filter_f['conf']) <= 75]\n",
        "  Df_f_2 = Df_filter_f[Df_filter_f.text.str.len() < 3]  \n",
        "  Df_f_3 = Df_f_1.join(Df_f_2, lsuffix='_caller', rsuffix='_other').dropna()\n",
        "  Df_filter1 = Df_filter_f.drop(Df_f_3.index.values.tolist())\n",
        "  Df_filter1 = Df_filter1.rename(columns={'block_num': 'block_n'})\n",
        "  add_no = pd.to_numeric(Df_filter1[\"block_n\"]) +last_blok\n",
        "  Df_filter1['block_num'] = add_no\n",
        "  Df_filter1 = Df_filter1.dropna()\n",
        "  next_first_blok = Df_filter1[\"block_num\"].iloc[0]\n",
        "  # print(Df_filter1) \n",
        "  return Df_filter1,next_first_blok                          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uU6WTrVOCQM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2zQBkCTKZbM"
      },
      "source": [
        "#return head an name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnI__1_EOsM6"
      },
      "source": [
        "def find_head(dff):\n",
        "  Df_filter = dff  \n",
        "  # Find key heading\n",
        "  key_word = [\"CONTACT\",\"Education\",\"Project\",\"EXPERIENCE\",\"Referees\",\"Skill\",\"About\",\"Award\",\"Professional\",\"INTEREST\",\n",
        "              \"ACTIVITIES\",\"AWARDS\",\"PUBLICATION\",\"Activity\",\"Hobbies\",\"Profile\",\"work\",\"QUALIFICATION\"]\n",
        "  awoid_key = [\"www\",\"@\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\",\"=\",\"_\",\",\",\"&\"]\n",
        "  # get guessed heading  \n",
        "  pattern = '|'.join(key_word)\n",
        "  guessed_heading_df =Df_filter[Df_filter['text'].str.contains(pattern,na=False, case=False)]\n",
        "  # get list of guessed head lock num\n",
        "  # print(guessed_heading_df[\"text\"].tolist())\n",
        "  block_num_h_guessd = guessed_heading_df[\"block_num\"].values.tolist()\n",
        "  # print(guessed_heading_df)\n",
        "\n",
        "  # awoid symbols\n",
        "  awoid_pattern = '|'.join(awoid_key)\n",
        "  df_awoid_symbol =Df_filter[Df_filter['text'].str.contains(awoid_pattern,na=False, case=False)] \n",
        "  # print(df_awoid_symbol[\"block_num\"].tolist())\n",
        "  # print(df_awoid_symbol[\"text\"])\n",
        "  df_awoid = Df_filter[~Df_filter['block_num'].isin(df_awoid_symbol[\"block_num\"].values.tolist())]\n",
        "  \n",
        "  # print(df_awoid[\"text\"].tolist())\n",
        "  # get the guessed heading block number\n",
        "    # get original (awoid symbol) block list\n",
        "  block_num_all = df_awoid[\"block_num\"].values.tolist()\n",
        "  # print(block_num_all)\n",
        "  # find raws based same block_num \n",
        "  full_heading_df = df_awoid[df_awoid['block_num'].isin(block_num_h_guessd)]\n",
        "  # print(full_heading_df[\"block_num\"].values.tolist())\n",
        "  # block number from original df\n",
        "  same_block_no=full_heading_df['block_num'].values.tolist()\n",
        "  # print(same_block_no)\n",
        "  # 1 or 2 blocks items (we assume heading must on 1 or 2 words) (get 1 or 2 block word from originl df)\n",
        "  must_head_blok_list = [item for item, count in collections.Counter(same_block_no).items() if (count <= 3)]\n",
        "  # print(must_head_blok_list)\n",
        "  # find df of must head \n",
        "  one_or_two_blok_heading1 = Df_filter[Df_filter['block_num'].isin(must_head_blok_list)]\n",
        "  # print(one_or_two_blok_heading1)\n",
        "\n",
        "  # ///////////// -----|^ find gessed all heading\n",
        "\n",
        "  \n",
        "  # key waigth////// in one_or_two_blok_heading \n",
        "  numaric_w = pd.to_numeric(df_awoid['width'])\n",
        "  numaric_h = pd.to_numeric(df_awoid['height'])\n",
        "  df_waigth = numaric_w * numaric_h\n",
        "  # charector count\n",
        "  D_text_count = df_awoid['text'].str.len()\n",
        "  df_key_waight_all =df_waigth / D_text_count\n",
        "  # print(df_key_waight_all)\n",
        "  # //////////\n",
        "  \n",
        "  \n",
        "\n",
        "  # /////////////// get gussed heading waight and find minimum number of k_waight \n",
        "  numaric_w_h = pd.to_numeric(one_or_two_blok_heading1['width'])\n",
        "  numaric_h_h = pd.to_numeric(one_or_two_blok_heading1['height'])\n",
        "  df_waigth_head = numaric_w_h * numaric_h_h\n",
        "  # print(df_waigth_all.values.tolist())\n",
        "  D_text_count_head = one_or_two_blok_heading1['text'].str.len()\n",
        "  df_key_waight_head = df_waigth_head / D_text_count_head\n",
        "  avg_key_waight = df_key_waight_head.min(skipna=True)\n",
        "  # print(avg_key_waight)\n",
        "  # ///////////////\n",
        "\n",
        "  # print(Df_filter[Df_filter['key waigth'] > 25860 ])\n",
        "  Df_waight_filter = df_key_waight_all[df_key_waight_all >= avg_key_waight] \n",
        "  # print(Df_waight_filter)\n",
        "\n",
        "  Df_waight_filter_text = (pd.concat([Df_waight_filter, df_awoid.reindex(Df_waight_filter.index)], axis=1)).dropna()\n",
        "  # print(Df_waight_filter_text)\n",
        "  \n",
        "  # 1 or 2 or 3 line contain heading : validate\n",
        "  block_num_h1 = Df_waight_filter_text[\"block_num\"].values.tolist()\n",
        "  # print(block_num_h1)\n",
        "\n",
        "  # must_head_blok_list = [item for item, count in collections.Counter(block_num_h1).items() if (count <= 3)]\n",
        "  # print(block_num_h1)\n",
        "  full_heading_df1 = Df_filter[Df_filter['block_num'].isin(block_num_h1)]\n",
        "  fewline_head = full_heading_df1[\"block_num\"].tolist()\n",
        "  # print(fewline_head)\n",
        "  must_head_blok_list = [item for item, count in collections.Counter(fewline_head).items() if (count <= 3)]\n",
        "  df_headind = Df_filter[Df_filter['block_num'].isin(must_head_blok_list)]\n",
        "\n",
        "\n",
        "\n",
        "  # NAME\n",
        "  # //////\n",
        "  df_len_for_name = len(Df_filter) / 3\n",
        "  Df_filter_name = Df_filter.loc[:df_len_for_name]\n",
        "  numaric_w = pd.to_numeric(Df_filter_name['width'])\n",
        "  numaric_h = pd.to_numeric(Df_filter_name['height'])\n",
        "  df_waigth = numaric_w * numaric_h\n",
        "  # charector count\n",
        "  D_text_count = Df_filter['text'].str.len()\n",
        "  df_key_waight_all =df_waigth / D_text_count\n",
        "  # //////\n",
        "  Df_waight_filter = df_key_waight_all[df_key_waight_all >= avg_key_waight] \n",
        "  # print(Df_waight_filter)\n",
        "  \n",
        "  Df_waight_filter_text = (pd.concat([Df_waight_filter, Df_filter_name], axis=1)).dropna()\n",
        "  \n",
        "  pattern = '|'.join(key_word)\n",
        "  guessed_heading_df =Df_waight_filter_text[~Df_waight_filter_text['text'].str.contains(pattern,na=False, case=False)]\n",
        "  awoid_pattern = '|'.join(awoid_key)\n",
        "  df_awoid_symbol =guessed_heading_df[~guessed_heading_df['text'].str.contains(awoid_pattern,na=False, case=False)]\n",
        "  fewline_head_name = df_awoid_symbol[\"block_num\"].tolist()\n",
        "  # print(df_awoid_symbol)\n",
        "  \n",
        "  # print(must_head_blok_list1)\n",
        "  must_head_blok_list2 = Df_filter_name[Df_filter_name['block_num'].isin(fewline_head_name)]\n",
        "  must_head_blok_list1 = [item for item, count in collections.Counter(must_head_blok_list2[\"block_num\"].tolist()).items() if (count <= 7)]\n",
        "  # print(must_head_blok_list1)\n",
        "  must_head_blok_list = Df_filter_name[Df_filter_name['block_num'].isin(must_head_blok_list1)]\n",
        "  # must_head_blok_list = (pd.merge_asof(df2, Df_filter_name,on=index))\n",
        "  # print(must_head_blok_list)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "# /////// big word find\n",
        "  numaric_w_n = pd.to_numeric(must_head_blok_list['width'])\n",
        "  numaric_h_n = pd.to_numeric(must_head_blok_list['height'])\n",
        "  df_waigth_n = numaric_w_n * numaric_h_n\n",
        "  # charector count\n",
        "  D_text_count = must_head_blok_list['text'].str.len()\n",
        "  df_key_waight_all = df_waigth_n / D_text_count\n",
        "  # print(df_key_waight_all)\n",
        "\n",
        "  must_head_blok_list3 = must_head_blok_list.loc[df_key_waight_all.index.tolist()]\n",
        "  # print(must_head_blok_list3)\n",
        "  pattern = '|'.join(key_word)\n",
        "  must_head_blok_list =must_head_blok_list3[~must_head_blok_list3['text'].str.contains(pattern,na=False, case=False)]\n",
        "  # print(must_head_blok_list)\n",
        "  df_key_waight_all1 = df_key_waight_all.loc[must_head_blok_list.index.values.tolist()]\n",
        "  name_index = df_key_waight_all1.idxmax()  \n",
        "  # print(df_key_waight_all1)\n",
        "  block_num = must_head_blok_list.loc[name_index].block_num\n",
        "  # print(block_num)\n",
        "  # find raws based same block_num \n",
        "  df2 =must_head_blok_list.loc[must_head_blok_list[\"block_num\"] == block_num]\n",
        "  # print(df2)\n",
        "  # get text of same raw and mearg as a string.\n",
        "  ln = df2['text'].values\n",
        "  print(' '.join(ln))\n",
        "  \n",
        "  # 1 or 2 or 3 line contain heading : validate\n",
        "  # block_num_h1 = Df_waight_filter_text[\"block_num\"].values.tolist()\n",
        "  # pattern = '|'.join(key_word)\n",
        "  # df_headind_n =df_headind[~df_headind['text'].str.contains(pattern,na=False, case=False)]  \n",
        "\n",
        "  return df_headind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUxEN_Pz9-3N"
      },
      "source": [
        "#find head (heading, no heading)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IglUh0nwato6"
      },
      "source": [
        "def details(Df_filter,Df_head,word):\n",
        "  # word = \"EXPERIENCE\"\n",
        "  head_index = Df_head[Df_head['text'].str.contains(word, na=False, case=False)]\n",
        "  # print(head_index)\n",
        "  if head_index.empty == True:\n",
        "    print(word,\" is not in the Heading list.\")\n",
        "    word_set = no_heading(Df_filter,Df_head,word)\n",
        "  else:\n",
        "    print(word, \"is in the Heading list.\")\n",
        "    word_set = heading(Df_filter,Df_head,word,head_index)\n",
        "\n",
        "  return word_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbnGFOVv9o2i"
      },
      "source": [
        "# heading = TRUE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VukqALZKRZD"
      },
      "source": [
        "# heading = TRUE\n",
        "def heading(Df_filter,Df_head,word,head_index):\n",
        "    \n",
        "  # get Details(df) between two heading\n",
        "  start_index = head_index.index.values[0] \n",
        "  df_head_index_list = Df_head.index.values.tolist()\n",
        "  end_index = df_head_index_list[(df_head_index_list.index(start_index)) % len(df_head_index_list)] \n",
        "\n",
        "  if end_index==start_index:\n",
        "    print(word,\" is last heading\")\n",
        "      # end_index = None\n",
        "      # end_index = int()\n",
        "\n",
        "    df_data_word = Df_filter.loc[start_index+1 : ]\n",
        "    \n",
        "    # get number of block count , between two heading(ovvoru blocklayum periya k_waightku uriya word.a eduthu, athu Key_wordku\n",
        "    # samana irukkanu paarthu = enda athodaye stop pannanum, illana adutha head vara vaasikanum)\n",
        "    # get block numbers\n",
        "    blok_tuple = (df_data_word[\"block_num\"].drop_duplicates())\n",
        "    # print(blok_tuple)\n",
        "    blok_len = len(blok_tuple)\n",
        "    # print(blok_len)\n",
        "    b = 0\n",
        "    while (b < blok_len):\n",
        "      blok_num=blok_tuple.iloc[b]\n",
        "      a_blok_text_set = df_data_word[df_data_word[\"block_num\"].isin([blok_num])]\n",
        "      numaric_w_w = pd.to_numeric(a_blok_text_set['width'])\n",
        "      numaric_h_w = pd.to_numeric(a_blok_text_set['height'])\n",
        "      df_waigth_word = numaric_w_w * numaric_h_w\n",
        "      D_text_count_head = a_blok_text_set['text'].str.len()\n",
        "      df_key_waight_word = df_waigth_word / D_text_count_head\n",
        "      max_key_waight = df_key_waight_word.idxmax()\n",
        "      max_w_word_a_blok = df_data_word[\"text\"].loc[[max_key_waight]]\n",
        "      # print(max_w_word_a_blok)\n",
        "      \n",
        "      key_word = [\"Education\",\"Project\",\"Referees\",\"Skill\",\"Award\",\"Professional\",\"INTEREST\",\n",
        "                  \"ACTIVITIES\",\"PUBLICATION\",\"Activity\",\"Hobbies\"]\n",
        "\n",
        "      pattern = '|'.join(key_word)\n",
        "      df_if_in_word = max_w_word_a_blok.str.contains(pattern,na=False, case=False).values[0]\n",
        "      # print(max_w_word_a_blok.str.contains(pattern,na=False, case=False))\n",
        "      if df_if_in_word == True:\n",
        "        end_index1 = max_w_word_a_blok.index.values[0] \n",
        "        df_data_word_finl = Df_filter.loc[start_index+1 : end_index1-1]\n",
        "        break \n",
        "      else:\n",
        "        df_data_word_finl = Df_filter.loc[start_index+1 :]   \n",
        "      b += 1\n",
        "\n",
        "  else:\n",
        "    print(word,\" is center heading\")\n",
        "    df_data_word = Df_filter.loc[start_index+1 : end_index-1 ]\n",
        "    \n",
        "    \n",
        "    # get number of block count , between two heading(ovvoru blocklayum periya k_waightku uriya word.a eduthu, athu Key_wordku\n",
        "    # samana irukkanu paarthu = enda athodaye stop pannanum, illana adutha head vara vaasikanum)\n",
        "    # get block numbers\n",
        "    blok_tuple = (df_data_word[\"block_num\"].drop_duplicates())\n",
        "    # print(blok_tuple)\n",
        "    blok_len = len(blok_tuple)\n",
        "    # print(blok_len)\n",
        "    b = 0\n",
        "    while (b < blok_len):\n",
        "      blok_num=blok_tuple.iloc[b]\n",
        "      a_blok_text_set = df_data_word[df_data_word[\"block_num\"].isin([blok_num])]\n",
        "      numaric_w_w = pd.to_numeric(a_blok_text_set['width'])\n",
        "      numaric_h_w = pd.to_numeric(a_blok_text_set['height'])\n",
        "      df_waigth_word = numaric_w_w * numaric_h_w\n",
        "      D_text_count_head = a_blok_text_set['text'].str.len()\n",
        "      df_key_waight_word = df_waigth_word / D_text_count_head\n",
        "      max_key_waight = df_key_waight_word.idxmax()\n",
        "      max_w_word_a_blok = df_data_word[\"text\"].loc[[max_key_waight]]\n",
        "      \n",
        "      key_word = [\"Education\",\"Project\",\"Referees\",\"Skill\",\"Award\",\"Professional\",\"INTEREST\",\n",
        "                  \"ACTIVITIES\",\"PUBLICATION\",\"Activity\",\"Hobbies\"]\n",
        "\n",
        "      pattern = '|'.join(key_word)\n",
        "      df_if_in_word = max_w_word_a_blok.str.contains(pattern,na=False, case=False).values[0]\n",
        "     \n",
        "      if df_if_in_word == True:\n",
        "        end_index1 = max_w_word_a_blok.index.values[0] \n",
        "        df_data_word_finl = Df_filter.loc[start_index+1 : end_index1-1]\n",
        "        break \n",
        "      else:\n",
        "        df_data_word_finl = Df_filter.loc[start_index+1 :end_index-1]   \n",
        "      b += 1\n",
        "\n",
        "\n",
        "  # df_data_word_finl = Df_filter.loc[start_index+1 : end_index]\n",
        "  ln = df_data_word_finl['text'].values\n",
        "  details_text = '\\n'.join(' '.join(ln).split('. '))\n",
        "  print(details_text)\n",
        "  return details_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa6gdTOp9hWC"
      },
      "source": [
        "# Heading Fales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP7T-yr9QHz0"
      },
      "source": [
        "# Heading Fales \n",
        "# word.a thedi eduthu, athila periya k_waight irukratha select panni\n",
        "# athukku aduthatha ulla head.a thedi eduththu, idaila ullatha df aakkanum, next ,Heading True.la ulla pola seiyanum.\n",
        "\n",
        "def no_heading(Df_filter,Df_head,word):\n",
        "\n",
        "  head_index_tuple = Df_filter[Df_filter['text'].str.contains(word, na=False, case=False)]\n",
        "  # print(head_index_tuple)\n",
        "\n",
        "  if head_index_tuple.empty:\n",
        "    print(word,\" is not in the cv\")\n",
        "    details_text = word+\" is not in the cv\"    \n",
        "\n",
        "  else:\n",
        "    numaric_w_w = pd.to_numeric(head_index_tuple['width'])\n",
        "    numaric_h_w = pd.to_numeric(head_index_tuple['height'])\n",
        "    df_waigth_word = numaric_w_w * numaric_h_w\n",
        "    D_text_count_head = head_index_tuple['text'].str.len()\n",
        "    df_key_waight_word = df_waigth_word / D_text_count_head\n",
        "    # print(df_key_waight_word)\n",
        "    max_key_waight = df_key_waight_word.idxmax()\n",
        "    # print(max_key_waight)\n",
        "    max_w_word_a_blok = Df_filter[\"text\"].loc[[max_key_waight]]\n",
        "\n",
        "    big_word_index = max_w_word_a_blok.index[0]\n",
        "    start_index = big_word_index\n",
        "    # find next head in Df_head\n",
        "      # find last head index\n",
        "    # print(Df_head[\"text\"].tolist())\n",
        "    head_index = Df_head.index.tolist() \n",
        "    last_head_index = Df_head.iloc[[-1]].index[0]\n",
        "    # print(last_head_index , big_word_index )\n",
        "    \n",
        "    # big word last head or belove to head\n",
        "    if last_head_index <= big_word_index:\n",
        "      df_data_word_bigword_l = Df_filter.loc[big_word_index+1 :]  \n",
        "      df_data_word_finl = Df_filter.loc[start_index+1 :]\n",
        "      print(\"word in last heading\")\n",
        "\n",
        "       # get number of block count , between two heading(ovvoru blocklayum periya k_waightku uriya word.a eduthu, athu Key_wordku\n",
        "      # samana irukkanu paarthu = enda athodaye stop pannanum, illana adutha head vara vaasikanum)\n",
        "      # get block numbers\n",
        "      blok_tuple = (df_data_word_bigword[\"block_num\"].drop_duplicates())\n",
        "      # print(df_data_word[df_data_word[\"block_num\"].isin([17])])\n",
        "      blok_len = len(blok_tuple)\n",
        "      # print(blok_len)\n",
        "      b = 0\n",
        "      while (b < blok_len):\n",
        "        blok_num=blok_tuple.iloc[b]\n",
        "        a_blok_text_set = df_data_word_bigword[df_data_word_bigword[\"block_num\"].isin([blok_num])]\n",
        "        numaric_w_w = pd.to_numeric(a_blok_text_set['width'])\n",
        "        numaric_h_w = pd.to_numeric(a_blok_text_set['height'])\n",
        "        df_waigth_word = numaric_w_w * numaric_h_w\n",
        "        D_text_count_head = a_blok_text_set['text'].str.len()\n",
        "        df_key_waight_word = df_waigth_word / D_text_count_head\n",
        "        max_key_waight = df_key_waight_word.idxmax()\n",
        "        max_w_word_a_blok = df_data_word_bigword[\"text\"].loc[[max_key_waight]]\n",
        "        \n",
        "        key_word = [\"Education\",\"Project\",\"Referees\",\"Skill\",\"Award\",\"Professional\",\"INTEREST\",\n",
        "                    \"ACTIVITIES\",\"PUBLICATION\",\"Activity\",\"Hobbies\"]\n",
        "\n",
        "        pattern = '|'.join(key_word)\n",
        "        df_if_in_word = max_w_word_a_blok.str.contains(pattern,na=False, case=False).values[0]\n",
        "        # print(df_if_in_word)\n",
        "        if df_if_in_word == True:\n",
        "          end_index1 = max_w_word_a_blok.index.values[0]\n",
        "          df_data_word_finl = Df_filter.loc[start_index+1 : end_index1-1]           \n",
        "          break  \n",
        "        else:\n",
        "          df_data_word_finl = Df_filter.loc[start_index+1 :]  \n",
        "        b +=1       \n",
        "        \n",
        "\n",
        "    else: \n",
        "      print(\"word in center area\")\n",
        "      next_head_bigword = [i for i in head_index if i > big_word_index][0]\n",
        "      # print(head_index)\n",
        "      # print(big_word_index+1 , next_head_bigword-1 )\n",
        "      df_data_word_bigword = Df_filter.loc[big_word_index+1 : next_head_bigword-1]\n",
        "      # print(df_data_word_bigword)\n",
        "\n",
        "      # get number of block count , between two heading(ovvoru blocklayum periya k_waightku uriya word.a eduthu, athu Key_wordku\n",
        "      # samana irukkanu paarthu = enda athodaye stop pannanum, illana adutha head vara vaasikanum)\n",
        "      # get block numbers\n",
        "      blok_tuple = (df_data_word_bigword[\"block_num\"].drop_duplicates())\n",
        "      # print(df_data_word[df_data_word[\"block_num\"].isin([17])])\n",
        "      blok_len = len(blok_tuple)\n",
        "      # print(blok_len)\n",
        "      b = 0\n",
        "      while (b < blok_len):\n",
        "        blok_num=blok_tuple.iloc[b]\n",
        "        a_blok_text_set = df_data_word_bigword[df_data_word_bigword[\"block_num\"].isin([blok_num])]\n",
        "        numaric_w_w = pd.to_numeric(a_blok_text_set['width'])\n",
        "        numaric_h_w = pd.to_numeric(a_blok_text_set['height'])\n",
        "        df_waigth_word = numaric_w_w * numaric_h_w\n",
        "        D_text_count_head = a_blok_text_set['text'].str.len()\n",
        "        df_key_waight_word = df_waigth_word / D_text_count_head\n",
        "        max_key_waight = df_key_waight_word.idxmax()\n",
        "        max_w_word_a_blok = df_data_word_bigword[\"text\"].loc[[max_key_waight]]\n",
        "        \n",
        "        key_word = [\"Education\",\"Project\",\"Referees\",\"Skill\",\"Award\",\"Professional\",\"INTEREST\",\n",
        "                    \"ACTIVITIES\",\"PUBLICATION\",\"Activity\",\"Hobbies\"]\n",
        "\n",
        "        pattern = '|'.join(key_word)\n",
        "        df_if_in_word = max_w_word_a_blok.str.contains(pattern,na=False, case=False).values[0]\n",
        "        # print(df_if_in_word)\n",
        "        if df_if_in_word == True:\n",
        "          end_index1 = max_w_word_a_blok.index.values[0] \n",
        "          df_data_word_finl = Df_filter.loc[start_index+1 : end_index1-1]\n",
        "          break \n",
        "        else:\n",
        "          end_index = next_head_bigword-1 \n",
        "          df_data_word_finl = Df_filter.loc[start_index+1 : end_index]  \n",
        "        b +=1\n",
        "        \n",
        "    # df_data_word_finl = Df_filter.loc[start_index+1 : end_index]\n",
        "    ln = df_data_word_finl['text'].values\n",
        "    details_text = '\\n'.join(' '.join(ln).split('. '))\n",
        "    # print(details_text) \n",
        "    return details_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrdr9s3M6dzy"
      },
      "source": [
        "#Front - html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61BCTL9A6cjO"
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, render_template\n",
        "text = '''\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "  <meta charset=\"UTF-8\">\n",
        "  <title>Upload your Cv as a Pdf</title>\n",
        "</head>\n",
        "\n",
        "<body>\n",
        "\n",
        "    <input id=\"fileupload\" type=\"file\" name=\"fileupload\" />\n",
        "    <button id=\"upload-button\" onclick=\"uploadFile()\"> Upload </button>\n",
        "    <h5>EXPERIENCE</h5>\n",
        "    <P id = \"EXPERIENCE\"></p>\n",
        "    <br/>\n",
        "    <h5>EDUCATION</h5>\n",
        "    <P id = \"Education\"></p>\n",
        "\n",
        "<script>\n",
        "async function uploadFile() {\n",
        "    let formData = new FormData();           \n",
        "    formData.append(\"file\", fileupload.files[0]);\n",
        "    let response = await fetch('/second', \n",
        "    { method: \"POST\", body: formData});    \n",
        "     \n",
        "    let data = await response.json()\n",
        "    document.getElementById(\"EXPERIENCE\").innerHTML = data.EXPERIENCE; \n",
        "    document.getElementById(\"Education\").innerHTML = data.Education;\n",
        "    console.log(response)\n",
        "    console.log(data)\n",
        "    return fileupload.files[0];\n",
        "    \n",
        "}\n",
        "</script>\n",
        "  \n",
        "\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "file = open(\"templates/text.html\",\"w\")\n",
        "file.write(text)\n",
        "file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0kfcgOVcCO-"
      },
      "source": [
        "#DF by OCR "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9TWITHc7jnY"
      },
      "source": [
        "import json\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pdf2image\n",
        "try:\n",
        "    from PIL import Image\n",
        "except ImportError:\n",
        "    import Image\n",
        "import pytesseract\n",
        "\n",
        "\n",
        "def pdf_to_img(pdf_file):\n",
        "    return pdf2image.convert_from_path(pdf_file)\n",
        "\n",
        "\n",
        "def ocr_core(file):\n",
        "    df_text = pytesseract.image_to_data(file)\n",
        "    return df_text\n",
        "\n",
        "def head_find(Df_filter1):\n",
        "    dff1 = Df_filter1\n",
        "    Df_head_e = pd.DataFrame([]) \n",
        "    df_head1=find_head(dff1) \n",
        "    Df_head_e = Df_head_e.append(df_head1)\n",
        "    return Df_head_e\n",
        "\n",
        "\n",
        "def print_pages(pdf_file):\n",
        "    images = pdf_to_img(pdf_file)\n",
        "    Df_filter_e = pd.DataFrame([])\n",
        "    # Df_head = pd.DataFrame([])\n",
        "    for pg, img in enumerate(images):\n",
        "        df_img = ocr_core(img)\n",
        "        # dff = save_df(df_img)\n",
        "        if Df_filter_e.empty == True:\n",
        "          df_fill1,next_first_blok=save_df(df_img,0) \n",
        "          Df_filter_e = Df_filter_e.append(df_fill1,ignore_index=True)\n",
        "        else:\n",
        "          last_blok = Df_filter_e[\"block_num\"].iloc[-1]+1\n",
        "          blok_add_no = last_blok - next_first_blok\n",
        "          df_fill1,next_first_blok=save_df(df_img,blok_add_no) \n",
        "          Df_filter_e = Df_filter_e.append(df_fill1,ignore_index=True)\n",
        "        \n",
        "    \n",
        "    Df_head = head_find(Df_filter_e)\n",
        "\n",
        "    word_set = []\n",
        "    word = [\"EXPERIENCE\",\"Education\"]\n",
        "    w=0\n",
        "    while (w < len(word)):\n",
        "      D_words = details(Df_filter_e,Df_head,word[w])\n",
        "      word_set.append(D_words)\n",
        "      w += 1\n",
        "    a_series = pd.Series(word_set, word)  \n",
        "    result = a_series.to_json(orient=\"index\")\n",
        "    parsed = json.loads(result)\n",
        "    word_json = json.dumps(parsed, indent=4)\n",
        "    print(word_json)\n",
        "    return word_json    \n",
        "    print(Df_head)\n",
        "    # print(Df_filter_e)\n",
        "\n",
        "    # Df_head.to_csv (r'/content/head.csv', index = False, header=True)\n",
        "    # Df_filter_e.to_csv (r'/content/data1.csv', index = False, header=True) \n",
        "       \n",
        "# print_pages('sample.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdeQCBzx7LmO"
      },
      "source": [
        "#main - front/json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV--d5-w7dMP"
      },
      "source": [
        "from flask_ngrok import run_with_ngrok \n",
        "from flask import Flask, render_template, request, redirect, url_for, render_template, send_from_directory\n",
        "from werkzeug.utils import secure_filename\n",
        "from flask import request, jsonify\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "@app.route('/')\n",
        "def text():\n",
        "  return render_template('text.html')\n",
        "\n",
        "@app.route('/second', methods = ['GET', 'POST'])\n",
        "def get_details():\n",
        "  if request.method == 'POST':\n",
        "        f = request.files['file']\n",
        "        name = f.save(f.filename)\n",
        "        file_path = '/content/'+f.filename\n",
        "        result = print_pages(file_path)\n",
        "        # print(\"oooooooooo\",result)\n",
        "        # res = 'file uploaded successfully' \n",
        "       \n",
        "        return  result\n",
        "\n",
        "     \n",
        "\t\t\n",
        "if __name__ == '__main__':\n",
        "  \n",
        "   app.run()\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2gMXiyZzA-f",
        "outputId": "eef9b2de-0beb-482e-a792-002a3f97e857"
      },
      "source": [
        "!https://github.com/VEENA9/Extracting-Details-from-cv.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: https://github.com/VEENA9/Extracting-Details-from-cv.git: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}