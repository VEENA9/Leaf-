{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arimac_by_me.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zRcgO7_cQ-oy"
      ],
      "toc_visible": true,
      "mount_file_id": "1tSh4xmTafMD88IbBM5rDQwoVr3GdvS5q",
      "authorship_tag": "ABX9TyPXhJWTt1kTX3Gx9pjtW8cH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VEENA9/Leaf-/blob/master/Ari_by_me.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOigv-x_b3wI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a49138-dcd3-47dd-861c-3d2dad2a1bba"
      },
      "source": [
        "!pip3 install Pillow\n",
        "!pip3 install pytesseract\n",
        "!pip3 install pdf2image\n",
        "!sudo apt-get install tesseract-ocr\n",
        "!apt-get install poppler-utils \n",
        "!pip install python-poppler"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.7/dist-packages (0.3.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pytesseract) (7.1.2)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.7/dist-packages (1.15.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pdf2image) (7.1.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.00~git2288-10f4998a-2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (0.62.0-2ubuntu2.12).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Collecting python-poppler\n",
            "  Using cached https://files.pythonhosted.org/packages/38/77/8f7b5c8ff2c0a7c3afbb3dc8d2342ef2e3ef48053a467e02913e18b73dc6/python-poppler-0.2.2.tar.gz\n",
            "Building wheels for collected packages: python-poppler\n",
            "  Building wheel for python-poppler (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for python-poppler\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for python-poppler\n",
            "Failed to build python-poppler\n",
            "Installing collected packages: python-poppler\n",
            "    Running setup.py install for python-poppler ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-sahr70kc/python-poppler/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-sahr70kc/python-poppler/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-zzrcs5mc/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9TWITHc7jnY"
      },
      "source": [
        "import pdf2image\n",
        "try:\n",
        "    from PIL import Image\n",
        "except ImportError:\n",
        "    import Image\n",
        "import pytesseract\n",
        "\n",
        "\n",
        "def pdf_to_img(pdf_file):\n",
        "    return pdf2image.convert_from_path('/content/drive/MyDrive/Example/Alagaretnam Elakyaveena.pdf')\n",
        "\n",
        "\n",
        "def ocr_core(file):\n",
        "    text = pytesseract.image_to_string(file)\n",
        "    return text\n",
        "\n",
        "\n",
        "def print_pages(pdf_file):\n",
        "    images = pdf_to_img(pdf_file)\n",
        "    for pg, img in enumerate(images):\n",
        "        print(ocr_core(img))\n",
        "\n",
        "\n",
        "p = print_pages('sample.pdf')\n",
        "print(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoOvKfSUKQbc"
      },
      "source": [
        "print(pytesseract.image_to_data(Image.open('test.png')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p66DoiMTPV1v"
      },
      "source": [
        "from PyPDF2 import PdfFileReader\n",
        "\n",
        "def extract_information(pdf_path):\n",
        "    with open(pdf_path, 'rb') as f:\n",
        "        pdf = PdfFileReader(f)\n",
        "        information = pdf.getDocumentInfo()\n",
        "        number_of_pages = pdf.getNumPages()\n",
        "\n",
        "    txt = f\"\"\"\n",
        "    Information about {pdf_path}: \n",
        "\n",
        "    Author: {information.author}\n",
        "    Creator: {information.creator}\n",
        "    Producer: {information.producer}\n",
        "    Subject: {information.subject}\n",
        "    Title: {information.title}\n",
        "    Number of pages: {number_of_pages}\n",
        "    \"\"\"\n",
        "\n",
        "    print(txt)\n",
        "    return information\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    path = '/content/drive/MyDrive/Example/ex_me.pdf'\n",
        "    extract_information(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsMLk_pS-uj1"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import sys\n",
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "import pandas as pd\n",
        "import collections\n",
        "\n",
        "# Path of the pdf\n",
        "PDF_file = \"/content/drive/MyDrive/Example/Chalani_Aththanayake_QA .pdf\"\n",
        "\n",
        "'''\n",
        "Part #1 : Converting PDF to images\n",
        "'''\n",
        "\n",
        "# Store all the pages of the PDF in a variable\n",
        "pages = convert_from_path(PDF_file, 500)\n",
        "\n",
        "# Counter to store images of each page of PDF to image\n",
        "image_counter = 1\n",
        "\n",
        "# Iterate through all the pages stored above\n",
        "for page in pages:\n",
        "\n",
        "\t# Declaring filename for each page of PDF as JPG\n",
        "\t# For each page, filename will be:\n",
        "\t# PDF page 1 -> page_1.jpg\n",
        "\t# PDF page 2 -> page_2.jpg\n",
        "\t# PDF page 3 -> page_3.jpg\n",
        "\t# ....\n",
        "\t# PDF page n -> page_n.jpg\n",
        "\tfilename = \"page_\"+str(image_counter)+\".jpg\"\n",
        "\t\n",
        "\t# Save the image of the page in system\n",
        "\tpage.save(filename, 'JPEG')\n",
        "\n",
        "\t# Increment the counter to update filename\n",
        "\timage_counter = image_counter + 1\n",
        "\n",
        "'''\n",
        "Part #2 - Recognizing text from the images using OCR\n",
        "'''\n",
        "\t# 3\n",
        "# Variable to get count of total number of pages\n",
        "filelimit = image_counter-1\n",
        "\n",
        "# Creating a text file to write the output\n",
        "outfile = \"out_text.txt\"\n",
        "\n",
        "# Open the file in append mode so that\n",
        "# All contents of all images are added to the same file\n",
        "f = open(outfile, \"a\")\n",
        "\n",
        "# Iterate from 1 to total number of pages\n",
        "for i in range(1, filelimit + 1):\n",
        "\n",
        "\t# Set filename to recognize text from\n",
        "\t# Again, these files will be:\n",
        "\t# page_1.jpg\n",
        "\t# page_2.jpg\n",
        "\t# ....\n",
        "\t# page_n.jpg\n",
        "\tfilename = \"page_\"+str(i)+\".jpg\"\n",
        "\t\t\n",
        "\t# Recognize the text as string in image using pytesserct\n",
        "\ttext = str(((pytesseract.image_to_string(Image.open(filename)))))\n",
        "\n",
        "\t# The recognized text is stored in variable text\n",
        "\t# Any string processing may be applied on text\n",
        "\t# Here, basic formatting has been done:\n",
        "\t# In many PDFs, at line ending, if a word can't\n",
        "\t# be written fully, a 'hyphen' is added.\n",
        "\t# The rest of the word is written in the next line\n",
        "\t# Eg: This is a sample text this word here GeeksF-\n",
        "\t# orGeeks is half on first line, remaining on next.\n",
        "\t# To remove this, we replace every '-\\n' to ''.\n",
        "\ttext = text.replace('-\\n', '')\t\n",
        "\n",
        "\t# Finally, write the processed text to the file.\n",
        "\tf.write(text)\n",
        "\n",
        "# Close the file after writing all the text.\n",
        "f.close()\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PibfYydrYD8Q"
      },
      "source": [
        "#name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKRTaB1wsffe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4501d1b4-c556-48ca-e0d0-9dc63d39a999"
      },
      "source": [
        "# get data from image\n",
        "table_Cv = pytesseract.image_to_data(Image.open(filename))\n",
        "# image data to DF\n",
        "table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "# print(table_Cv_df)\n",
        "table_Cv = pytesseract.image_to_data(Image.open(filename))\n",
        "# image data to DF\n",
        "table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "# add headera same as 1st row\n",
        "table_Cv_df1 = table_Cv_df.rename(columns=table_Cv_df.iloc[0])\n",
        "# remove 1st row\n",
        "table_Cv_df1  = table_Cv_df1[1:]\n",
        "\n",
        "# filter DF (empty text, left and top 0, remove \"e\":line)\n",
        "Df_filter = table_Cv_df1.loc[(table_Cv_df1.text != ' ') & (table_Cv_df1.left != '0') & \n",
        "                             (table_Cv_df1.top != '0')& (table_Cv_df1.text != 'e')& (table_Cv_df1.text != \"\")]\n",
        "name = (Df_filter.width) \n",
        "Df_filter.to_csv (r'/content/amilD.csv', index = False, header=True)\n",
        "# get high widht size as it is a NAME\n",
        "big_word =(max(Df_filter.width.astype(float)))\n",
        "# get the big size word block number\n",
        "block_num = Df_filter[\"block_num\"].iloc[Df_filter[\"width\"].astype(float).argmax()]\n",
        "# find raws based same block_num \n",
        "df2 =Df_filter.loc[Df_filter[\"block_num\"] == block_num]\n",
        "# get text of same raw and mearg as a string.\n",
        "ln = df2['text'].values\n",
        "print(' '.join(ln))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ah Od Jf Internship — Atlink Communication Pvt Ltd (From August\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRcgO7_cQ-oy"
      },
      "source": [
        "#word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "jk8WyisNxveS",
        "outputId": "c7ba915c-1cb9-4e17-9bce-ab6a697b4b8d"
      },
      "source": [
        "#find the word, get that row tuple\n",
        "# word = \"Experience\"\n",
        "word = \"Education\"\n",
        "# find word tuple\n",
        "df_tuple_word=Df_filter[Df_filter['text'].str.contains(word, na=False, case=False)]\n",
        "# index of the word in original DF and next index of filter df////////////////\n",
        "# array of filtred df index\n",
        "place_num =Df_filter.index.values\n",
        "# index of original df\n",
        "word_index = df_tuple_word.index.values[0]\n",
        "# original index number on array\n",
        "x = np.where(place_num == word_index)\n",
        "# next to filterd df index of word in array\n",
        "index_new = x[0][0] + 1\n",
        "# index of original df\n",
        "next_indx = place_num[index_new]\n",
        "# block number of next to word bloc number\n",
        "next_blok_num = Df_filter.block_num[next_indx]\n",
        "# print(next_blok_num)\n",
        "# if heading and details in same block (check next raw is in same with word block number)\n",
        "if (int(next_blok_num) == int(df_tuple_word['block_num'])):\n",
        "  df3_b_num = int(df_tuple_word['block_num']) \n",
        "  \n",
        "else:\n",
        "  # sum one to find next raw num (experiance details in start from next line of experiance)\n",
        "  df3_b_num = int(df_tuple_word['block_num']) + 1\n",
        "# get df that: experiance data in same block number\n",
        "df_same_bloc_num =df_tuple_word.loc[df_tuple_word[\"block_num\"] == str(df3_b_num)]\n",
        "print(df_same_bloc_num)\n",
        "# get totel line of expariance\n",
        "# print(df_same_bloc_num.par_num.astype(float))\n",
        "try:\n",
        "  max_line_num = max(df_same_bloc_num.par_num.astype(float))\n",
        "except:\n",
        "  max_line_num = 1\n",
        "\n",
        "# loop for get line by line basd on par_num\n",
        "p_n = 1\n",
        "print(max_line_num)\n",
        "while (p_n <= max_line_num):\n",
        "  # creat df : for one line (filter based same par_num)\n",
        "  df_par_num =df_same_bloc_num.loc[df_same_bloc_num[\"par_num\"] == str(p_n)]\n",
        "  # add word of same par_num\n",
        "  ln2 = df_par_num['text'].values\n",
        "  print(' '.join(ln2))\n",
        "  p_n =p_n+1;  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8703a276d4e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Education\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# find word tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_tuple_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDf_filter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDf_filter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# index of the word in original DF and next index of filter df////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# array of filtred df index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Df_filter' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE_h0egGOP0g"
      },
      "source": [
        "#findHead Algo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpgldlvaW2FV"
      },
      "source": [
        "table_Cv = pytesseract.image_to_data(Image.open(filename))\n",
        "# image data to DF\n",
        "table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "# print(table_Cv_df)\n",
        "table_Cv = pytesseract.image_to_data(Image.open(filename))\n",
        "# image data to DF\n",
        "table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "# add headera same as 1st row\n",
        "table_Cv_df1 = table_Cv_df.rename(columns=table_Cv_df.iloc[0])\n",
        "# remove 1st row\n",
        "table_Cv_df1  = table_Cv_df1[1:]\n",
        "\n",
        "# filter DF (empty text, left and top 0, remove \"e\":line)\n",
        "Df_filter = table_Cv_df1.loc[(table_Cv_df1.text != ' ') & (table_Cv_df1.left != '0') & \n",
        "                             (table_Cv_df1.top != '0')& (table_Cv_df1.text != 'e')& (table_Cv_df1.text != \"\")]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HKMayQvLRK8"
      },
      "source": [
        "Df_filter_w = pd.to_numeric(Df_filter['width'])\n",
        "Df_filter_h = pd.to_numeric(Df_filter['height'])\n",
        "D_text_count = Df_filter['text'].str.len()\n",
        "\n",
        "print(Df_filter_h,Df_filter_w,D_text_count)\n",
        "print(Df_filter_w * Df_filter_h / D_text_count)\n",
        "# Df_filter['key waigth'] = Df_filter['wigth_n'] * Df_filter['height_n'] / Df_filter['text'].str.len()\n",
        "# key_waight_list = Df_filter['key waigth'].values.tolist()\n",
        "# print(key_waight_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnI__1_EOsM6",
        "outputId": "f73ad2c2-97b9-482d-876a-0de25a29ee04"
      },
      "source": [
        "# Find key heading\n",
        "key_word = [\"Contact\",\"Education\",\"Project\",\"Experience\",\"Referees\",\"Skill\",\"About\",\"Award\",\"Professional\",\"INTEREST\",\"ACTIVITIES\"]\n",
        "awoid_key = [\"www\",\"@\",\"com\",\"/\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\"]\n",
        "# get guessed heading  \n",
        "pattern = '|'.join(key_word)\n",
        "guessed_heading_df =Df_filter[Df_filter['text'].str.contains(pattern,na=False, case=False)]\n",
        "print(guessed_heading_df)\n",
        "# get the guessed heading block number\n",
        "block_num_h = guessed_heading_df[\"block_num\"].values.tolist()\n",
        "print(block_num_h)\n",
        "# find raws based same block_num \n",
        "full_heading_df = Df_filter[Df_filter['block_num'].isin(block_num_h)]\n",
        "print(full_heading_df['block_num'].values.tolist())\n",
        "# block number from original df\n",
        "same_block_no=full_heading_df['block_num'].values.tolist()\n",
        "# print(same_block_no)\n",
        "# 1 or 2 blocks items (we assume heading must on 1 or 2 words) (get 1 or 2 block word from originl df)\n",
        "must_head_blok_list = [item for item, count in collections.Counter(same_block_no).items() if (count <= 2)]\n",
        "print(must_head_blok_list)\n",
        "# find df of must head \n",
        "one_or_two_blok_heading = Df_filter[Df_filter['block_num'].isin(must_head_blok_list)]\n",
        "# print(one_or_two_blok_heading)\n",
        "\n",
        "# key waigth////// in one_or_two_blok_heading \n",
        "one_or_two_blok_heading_w = pd.to_numeric(one_or_two_blok_heading['width'])\n",
        "one_or_two_blok_heading_h = pd.to_numeric(one_or_two_blok_heading['height'])\n",
        "# print(one_or_two_blok_heading_h.values.tolist())\n",
        "df_waigth = one_or_two_blok_heading_w * one_or_two_blok_heading_h\n",
        "# print(df_waigth.values.tolist())\n",
        "D_text_count = one_or_two_blok_heading['text'].str.len()\n",
        "# print(D_text_count.values.tolist())\n",
        "df_key_waight =df_waigth / D_text_count\n",
        "\n",
        "#  =one_or_two_blok_heading_w * one_or_two_blok_heading_h / D_text_count\n",
        "# key_waight_list = Df_filter['key waigth'].values.tolist()\n",
        "# print(Df_filter)\n",
        "# get avg of must_head waight\n",
        "# avg_key_waight = sum(one_or_two_blok_heading[\"key waigth\"]) / len(one_or_two_blok_heading[\"key waigth\"])\n",
        "avg_key_waight = df_key_waight.min(skipna=True)\n",
        "# print(avg_key_waight)\n",
        "\n",
        "# ///////////////\n",
        "Df_filter_w = pd.to_numeric(Df_filter['width'])\n",
        "Df_filter_h = pd.to_numeric(Df_filter['height'])\n",
        "df_waigth_all = Df_filter_w * Df_filter_h\n",
        "# print(df_waigth_all.values.tolist())\n",
        "D_text_count_all = Df_filter['text'].str.len()\n",
        "df_key_waight_all =df_waigth_all / D_text_count_all\n",
        "# print(df_key_waight_all)\n",
        "# ///////////////\n",
        "\n",
        "\n",
        "# print(Df_filter[Df_filter['key waigth'] > 25860 ])\n",
        "Df_waight_filter = df_key_waight_all > avg_key_waight \n",
        "\n",
        "Df_waight_filter_text = pd.concat([Df_waight_filter, Df_filter.reindex(Df_waight_filter.index)], axis=1)\n",
        "\n",
        "# print(Df_waight_filter_text)\n",
        "\n",
        "# awoid kewywords\n",
        "awoid_pattern = '|'.join(awoid_key)\n",
        "awoiding_heading_df =Df_waight_filter_text[Df_waight_filter_text['text'].str.contains(awoid_pattern,na=False, case=False)]\n",
        "# print(awoiding_heading_df)\n",
        "\n",
        "df_finel_all_blok = pd.concat([Df_waight_filter_text, awoiding_heading_df, awoiding_heading_df]).drop_duplicates(keep=False)\n",
        "# print(df_finel_all_blok)\n",
        "\n",
        "# 1 or 2 or 3 line contain heading : validate\n",
        "block_num_h1 = df_finel_all_blok[\"block_num\"].values.tolist()\n",
        "# print(block_num_h1)\n",
        "full_heading_df1 = Df_filter[Df_filter['block_num'].isin(block_num_h1)]\n",
        "# print(full_heading_df['block_num'].values.tolist())\n",
        "# block number from original df\n",
        "same_block_no1=full_heading_df1['block_num'].values.tolist()\n",
        "# print(same_block_no)\n",
        "# 1 or 2 blocks items (we assume heading must on 1 or 2 words) (get 1 or 2 block word from originl df)\n",
        "head_blok_list = [item for item, count in collections.Counter(same_block_no1).items() if (count <= 3)]\n",
        "# print(head_blok_list)\n",
        "\n",
        "df_finl = Df_filter[Df_filter['block_num'].isin(head_blok_list)]\n",
        "print(df_finl)\n",
        "Df_filter.to_csv (r'/content/amilchal.csv', index = False, header=True)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    level page_num block_num par_num  ... width height conf         text\n",
            "6       5        1         1       1  ...   337     56   89       AWARDS\n",
            "10      5        1         1       1  ...   517     72   96     PROJECTS\n",
            "20      5        1         2       2  ...   349     46   86  Educational\n",
            "28      5        1         2       2  ...   260     66   96     project:\n",
            "75      5        1         2       3  ...   189     52   96      project\n",
            "99      5        1         2       3  ...   309     52   96  experience,\n",
            "133     5        1         4       1  ...   227     52   96    projects.\n",
            "154     5        1         5       2  ...   265     66   96     project:\n",
            "256     5        1        11       1  ...   307     52   95  experience.\n",
            "263     5        1        12       1  ...   195     46   95       skills\n",
            "270     5        1        12       1  ...   263     66   95     project:\n",
            "278     5        1        12       1  ...   198     46   96       skills\n",
            "298     5        1        12       2  ...   444     59   96    INTERESTS\n",
            "330     5        1        15       1  ...   617     73   96   EXPERIENCE\n",
            "374     5        1        17       2  ...   202     56   96      Project\n",
            "435     5        1        22       1  ...   400     55   95     REFEREES\n",
            "\n",
            "[16 rows x 12 columns]\n",
            "['1', '1', '2', '2', '2', '2', '4', '5', '11', '12', '12', '12', '12', '15', '17', '22']\n",
            "['1', '1', '1', '1', '1', '1', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '4', '4', '4', '4', '4', '4', '4', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '11', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '12', '15', '15', '15', '15', '15', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '17', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22', '22']\n",
            "['11']\n",
            "    level page_num block_num par_num  ... width height  conf         text\n",
            "256     5        1        11       1  ...   307     52    95  experience.\n",
            "321     5        1        14       1  ...   323     59    96   Organizing\n",
            "322     5        1        14       1  ...   193     45    96       events\n",
            "393     5        1        18       1  ...   276     58    95     Watching\n",
            "394     5        1        18       1  ...   202     45    96       Movies\n",
            "531           None      None    None  ...  None   None  None         None\n",
            "\n",
            "[6 rows x 12 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arDDFvo1sznW",
        "outputId": "8b3d4e7b-61ad-491d-8aa5-228693974cac"
      },
      "source": [
        "df_last_values = one_or_two_blok_heading[\"key waigth\"]\n",
        "print(df_last_values)\n",
        "Data_point_m= df_last_values.rolling(2, min_periods=1).mean()\n",
        "Data_point_m_D = Data_point_m.squeeze().tolist()\n",
        "round_to_whole = [round(num) for num in Data_point_m_D]\n",
        "print(round_to_whole)\n",
        "may_reduse=((sum(round_to_whole) / len(round_to_whole)) *0.25)\n",
        "moving_avg = ((sum(round_to_whole) / len(round_to_whole)))\n",
        "may_lower = moving_avg - may_reduse\n",
        "if (may_lower < min(df_last_values)):\n",
        "  avg_key_waight = min(df_last_values)\n",
        "else:\n",
        "  avg_key_waight = min(round_to_whole)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47     3914.857143\n",
            "133    3306.666667\n",
            "163    3775.111111\n",
            "212    3539.166667\n",
            "213    3472.000000\n",
            "276    3232.500000\n",
            "401    3572.000000\n",
            "402    3012.000000\n",
            "Name: key waigth, dtype: float64\n",
            "[3915, 3611, 3541, 3657, 3506, 3352, 3402, 3292]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzNDWA5tuapi",
        "outputId": "71a9fb12-5344-49fe-c5ef-6bd02d50e0e3"
      },
      "source": [
        "df = pd.DataFrame({'A': [5,6,3,4], 'B': [1,2,3,5]})\n",
        "print(df['A'])\n",
        "print(df[df['A'].isin([3, 6])])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    5\n",
            "1    6\n",
            "2    3\n",
            "3    4\n",
            "Name: A, dtype: int64\n",
            "   A  B\n",
            "1  6  2\n",
            "2  3  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWkeZOUcTi3o",
        "outputId": "1b4b8bcb-f27a-47c9-e596-d2c3f76c275b"
      },
      "source": [
        "print(Df_filter[Df_filter['text'].str.contains(\"EDUCATION\",na=False, case=False)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    level page_num block_num par_num  ... width height conf       text\n",
            "163     5        1        22       1  ...   548     62   96  EDUCATION\n",
            "\n",
            "[1 rows x 12 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hldDUn3LEBHp"
      },
      "source": [
        "#save df1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUqphpCeyqnS"
      },
      "source": [
        "table_Cv = pytesseract.image_to_data(Image.open(filename))\n",
        "# print(table_Cv.shape)\n",
        "table_Cv_df = pd.DataFrame([x.split('\\t') for x in table_Cv.split('\\n')])\n",
        "print(table_Cv_df)\n",
        "table_Cv_df.to_csv (r'/content/export_dataframe.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ji9Blw_KcpJ"
      },
      "source": [
        "print(pytesseract.image_to_data(Image.open(filename)))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}